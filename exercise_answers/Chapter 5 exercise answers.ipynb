{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction answers\n",
    "\n",
    "Notebook containing the example answers to Chapter 5 - Clustering.\n",
    "\n",
    "Note: these answers are examples and your way of tackling the problem may also be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Please run this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Using the data imported below, visualise X. \n",
    "\n",
    "Without looking at the true label values perform K-means clustering on the data with $K=2$. \n",
    "\n",
    "Visualise the results of the clustering labels assigned as shown in previous examples. Change the value of $K$ to a more appropriate value and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and separate data\n",
    "unknown_clusters_data = np.loadtxt(\"../data/exercise_one_clusters.csv\", delimiter=\",\")\n",
    "X, _true_labels= unknown_clusters_data[:,0:-1], unknown_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Visualise X\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the data without class labels to understand it's structure\n",
    "plt.scatter(x=X[:,0], y=X[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X has two dimensions of data, there are some clear groupings of data points, and other areas where potential groupings are near each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the clustering, produce group labels\n",
    "\n",
    "# Create and fit the clustering\n",
    "kmeans_2_clusters = KMeans(n_clusters=2).fit(X)\n",
    "\n",
    "# Access the found labels\n",
    "found_clusters = kmeans_2_clusters.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise your clustering results\n",
    "\n",
    "plt.title(\"K=2 Found Labels\")\n",
    "plt.scatter(x=X[:,0], y=X[:,1], c=found_clusters);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is quite a poor clustering. We have split the data into two labelled classes, which doesn't appear appropriate for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a better value of K\n",
    "# Repeating the code above and changing the value of `n_clusters`, visualising the results.\n",
    "\n",
    "K = 5\n",
    "\n",
    "kmeans_2_clusters = KMeans(n_clusters=K).fit(X)\n",
    "\n",
    "found_clusters = kmeans_2_clusters.labels_\n",
    "\n",
    "plt.scatter(x=X[:,0], y=X[:,1], c=found_clusters);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without using a way to evaluate the performance of a clustering, we can only really visualise and compare groupings using our intuition. Using just `X` and KMeans clustering there appears to be five different clusters in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Using the data imported below and the ARI work out what the best value for $K$ is. Plot the results of the best $K$. \n",
    "\n",
    "Using the true labels work out how many actual groups in the data there are, is this the same as what you found? Plot the true labels and compare results.\n",
    "\n",
    "**HINT** The np.unique() function gives you the unique elements in an array. Remember, the exact numerical label of each cluster isn't important, but whether points belong to the same group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and separate data\n",
    "unknown_clusters_data = np.loadtxt(\"../data/exercise_two_clusters.csv\", delimiter=\",\")\n",
    "X, true_labels= unknown_clusters_data[:,0:-1], unknown_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "# Find the best value for K using ARI\n",
    "\n",
    "# Create list to store value of K and associate ARI score\n",
    "ARI_scores = []\n",
    "\n",
    "# Generate values of K to be looped through\n",
    "K_values = range(1, 20)\n",
    "\n",
    "# Loop though values of K, creating new model and evaluating for each\n",
    "for K in K_values:\n",
    "    KMeans_model = KMeans(n_clusters=K).fit(X)\n",
    "    found_clusters = KMeans_model.labels_\n",
    "    # Calculate evaluation score\n",
    "    ARI_score = adjusted_rand_score(true_labels, found_clusters)\n",
    "    # Store K and ARI\n",
    "    ARI_scores.append((K, ARI_score))\n",
    "\n",
    "# Find the highest value ARI score and associated K\n",
    "best_K, best_score = max(ARI_scores, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best K value:\", best_K)\n",
    "print(\"ARI of best K:\", round(best_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the found labels for your value of K\n",
    "\n",
    "best_KMeans_model = KMeans(n_clusters=best_K_values[0]).fit(X)\n",
    "best_found_clusters = best_KMeans_model.labels_\n",
    "\n",
    "plt.scatter(x=X[:,0], y=X[:,1], c=best_found_clusters);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clustering looks very reasonable, with each group compact and separate.\n",
    "\n",
    "It would also be useful to look at the different ARI scores of each value of $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many true labels there are\n",
    "unique_true_labels = np.unique(true_labels)\n",
    "\n",
    "# Given the unique labels, how many labels is the length of the array\n",
    "print(\"Number of true classes:\", len(unique_true_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is close to the number of classes we found using ARI, but not exactly the right number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12,5), ncols=2, nrows=1, sharey=True)\n",
    "\n",
    "axes[0].set_title(\"True Labels\")\n",
    "axes[0].scatter(x=X[:,0], y=X[:,1], c=true_labels, cmap=\"magma\")\n",
    "\n",
    "axes[1].set_title(\"Best Found Labels\")\n",
    "axes[1].scatter(x=X[:,0], y=X[:,1], c=best_found_clusters, cmap=\"viridis\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see two main things from this comparision.\n",
    "\n",
    "Firstly, it shows why we have such a high ARI value, our clustering is getting most of the groupings correct. In each cluster, there are some errors or miss-grouped selections around edges, this is natural.\n",
    "\n",
    "Secondly we can see why we have found 7 clusters instead of 8. In the True Labels figure there are two clusters that are occupying a significant amount of common space. Without knowing the true labels it is very difficult for an algorithm to differentiate between these two clusters. Some methods are better at this than others.\n",
    "\n",
    "The effect this may have on our analysis is underestimating the number of groups in our data set. More importantly as a result we may lose two unique group in our analysis, and instead have one larger group.\n",
    "\n",
    "For example if we were clustering people's exercise habits, we could end up finding the group \"people who play racket sports\" rather than \"people who play tennis\" and \"people who play table tennis\".\n",
    "\n",
    "Without the true label values, it would be challenging to work this out. This is where domain knowledge and a thorough understanding of the data come into play. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Using the data imported below, perform KMeans clustering with $K=5$, calculate the silouette score using both manhattan and euclidean distances.\n",
    "\n",
    "Plot both found label sets, are there significant differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "unknown_clusters_data = np.loadtxt(\"../data/exercise_three_clusters.csv\", delimiter=\",\")\n",
    "X, _true_labels = unknown_clusters_data[:,0:-1], unknown_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "# Perform Kmeans clustering on the data.\n",
    "\n",
    "KMeans_model_5 = KMeans(n_clusters=5).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels and silhouette score using both metrics\n",
    "found_clusters = KMeans_model_5.labels_\n",
    "\n",
    "# Score using each metric\n",
    "sil_score_euc = silhouette_score(X=X, labels=found_clusters, metric=\"euclidean\")\n",
    "sil_score_man = silhouette_score(X=X, labels=found_clusters, metric=\"manhattan\")\n",
    "\n",
    "print(\"Euclidean Silhouette Score:\", round(sil_score_euc, 4))\n",
    "print(\"Manhattan Silhouette Score:\", round(sil_score_man, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst we do not select a distance metric to perform the KMeans clustering itself, we can do so when evaluating our clusterings intrinsically. In this case, there is not a significant difference between the scores of Manhattan and Euclidean metrics, but as have more complex data this may not always be true. As with all modelling packages it is important to check what the default parameters are before relying on the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Using the data loaded below, using *eps=0.15* calculate the silhouette score of the DBSCAN clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "swirl_data = np.loadtxt(\"../data/exercise_four_clusters.csv\", delimiter=\",\")\n",
    "X, _true_labels= swirl_data[:,0:-1], swirl_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "\n",
    "# Define the value for eps\n",
    "reach_dist = 0.15\n",
    "\n",
    "# Create and fit the model\n",
    "dbscan_clusterer = DBSCAN(eps=reach_dist, metric=\"euclidean\").fit(X)\n",
    "found_clusters = dbscan_clusterer.labels_\n",
    "\n",
    "# Score the found labels\n",
    "sil_score_euc = silhouette_score(X=X, labels=found_clusters, metric=\"euclidean\")\n",
    "\n",
    "print(\"eps={} silhouette score: {:.4f}\".format(reach_dist, sil_score_euc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the found clusters\n",
    "plt.scatter(x=X[:,0], y=X[:,1], c=found_clusters);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to be a well grouped clustering, however, there is a low silhouette score. This is a result of the choice in evaluation metric we have used. A high silhouette score favours compact, well distanced clusters. Because our data classes are *not well distanced or compact*, even the perfect clustering assignment would have a poor silhouette score.\n",
    "\n",
    "This is one reason why selecting an appropriate measurement is important. In addition, we should measure model performances *relative to each other and with the data in mind*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Using the data loaded below, visualise the two true classes in the data set, why might these clusters be challenging to model?\n",
    "\n",
    "Select a distance metric to use, then find an appropriate value of *eps* to use with DBSCAN using an appropriate evaluation method. Why did you select this distance metric?\n",
    "\n",
    "**HINT:** Check how many dimensions are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "further_swirl_data = np.loadtxt(\"../data/exercise_five_clusters.csv\", delimiter=\",\")\n",
    "X, true_labels= further_swirl_data[:,0:-1], further_swirl_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "# Visualise the data\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(12,5), ncols=2)\n",
    "\n",
    "axes[0].scatter(x=X[:,0], y=X[:,1], c=true_labels, cmap=\"coolwarm\")\n",
    "axes[1].scatter(x=X[:,0], y=X[:,2], c=true_labels, cmap=\"coolwarm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first figure, the two classes are highly intertwinned. In addition the have similar densities and are not well separated, with wide spread. \n",
    "\n",
    "The second image however shows that the data is quite well separated in the X[:,2] dimensions (third column of data), this could be useful to group the classes.\n",
    "\n",
    "It is important that we use all of the data at our disposal to group the data. \n",
    "\n",
    "Having tweaked the value of `eps` for both \"euclidean\" and \"manhattan\" distances, they can both find an optimal value of `eps` maximising performance. An important point to note is that the `eps` found for each distance metric will be different, as the reach distance is a different quantity.\n",
    "\n",
    "Which to pick here is a bit of a trick question, there are use cases for manhattan distance, but by default we tend to stick to euclidean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First explore the sensitivity to `eps` the data has and visualise the results. \n",
    "# We can optimise on some evaluation score shortly\n",
    "\n",
    "# Feel free to change `reach_dist` to explore impact of value\n",
    "reach_dist = 0.25\n",
    "\n",
    "dbscan_clusterer = DBSCAN(eps=reach_dist, metric=\"euclidean\").fit(X)\n",
    "found_clusters = dbscan_clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12,5), ncols=2)\n",
    "\n",
    "axes[0].scatter(x=X[:,0], y=X[:,1], c=found_clusters, cmap=\"plasma\")\n",
    "axes[1].scatter(x=X[:,0], y=X[:,2], c=found_clusters, cmap=\"plasma\");\n",
    "\n",
    "# Count how many unlabelled data pounts exist in the data set\n",
    "# Unlabelled data == -1, we can create a mask then count the True's (1's)\n",
    "print(\"Number unlabelled\", np.count_nonzero(found_clusters == -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage selecting different values of `eps` manually to find an appropriate performing reaching distance.\n",
    "\n",
    "For this task, the Adjusted Rand Index would be helpful, as we have the true labels. However, as we typically cluster when we do not have a true label value, we are going to use an intrinsic method instead.\n",
    "\n",
    "As shown in the previous exercises, the silhouette score performs poorly when the data groups are not compact (convex). This, unfortunately is an inherent problem with density based clustering. We should still be able to find the best value. For more evaluation options in `sklearn` [look at the documentation](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of eps values\n",
    "eps_values = np.linspace(start=0.01, stop=1, num=200)\n",
    "\n",
    "# A list to store eps values and associate scores\n",
    "sil_scores = []\n",
    "\n",
    "# Specify the metric\n",
    "distance_metric = \"manhattan\"\n",
    "\n",
    "# Loop over all eps values defined\n",
    "for reach_dist in eps_values:\n",
    "    \n",
    "    DBSCAN_model = DBSCAN(eps=reach_dist, metric=distance_metric).fit(X)\n",
    "    found_clusters = DBSCAN_model.labels_\n",
    "    # silhouette score only defined when more than one cluster found\n",
    "    if len(np.unique(found_clusters)) > 1:\n",
    "        sil_score = silhouette_score(X, found_clusters)\n",
    "        sil_scores.append((reach_dist, sil_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find eps with highest associated silhouette score\n",
    "highest_eps, highest_value = max(sil_scores, key=lambda x:x[1])\n",
    "\n",
    "print(\"Best value of eps:\", round(highest_eps, 4))\n",
    "print(\"Best eps Silhouette Score:\", round(highest_value, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_DBSCAN = DBSCAN(eps=highest_eps, metric=distance_metric).fit(X)\n",
    "best_found_clusters = best_DBSCAN.labels_\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(12,5), ncols=2)\n",
    "\n",
    "axes[0].scatter(x=X[:,0], y=X[:,1], c=best_found_clusters, cmap=\"copper\")\n",
    "axes[1].scatter(x=X[:,0], y=X[:,2], c=best_found_clusters, cmap=\"copper\");\n",
    "\n",
    "print(\"Number unlabelled\", np.count_nonzero(best_found_clusters == -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this optimal value of `eps` found separates the two clusters very well for what we wanted, despite the low silhouette values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "Using the \"three_clusters_data\" imported below, cluster the data with $n\\_clusters=3$ using both single and complete-linkage with the AgglomerativeClustering model. For both results, calculate the silhouette score. Which is higher? Why do you think this is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "three_clusters_data = np.loadtxt(\"../data/clusters.csv\", delimiter=\",\")\n",
    "X, true_labels = three_clusters_data[:,0:-1], three_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "\n",
    "# Lets first view our data\n",
    "plt.scatter(x=X[:,0], y=X[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model for each linkage method\n",
    "single_linkage_model = AgglomerativeClustering(n_clusters=3, linkage=\"single\").fit(X)\n",
    "complete_linkage_model = AgglomerativeClustering(n_clusters=3, linkage=\"complete\").fit(X)\n",
    "\n",
    "# Access labels assigned for each method\n",
    "single_clusters = single_linkage_model.labels_\n",
    "complete_clusters = complete_linkage_model.labels_\n",
    "\n",
    "# Evaluate performance of each model and ground truth\n",
    "single_silhouette = silhouette_score(X, single_clusters)\n",
    "complete_silhouette = silhouette_score(X, complete_clusters)\n",
    "true_silhouette = silhouette_score(X, true_labels)\n",
    "\n",
    "\n",
    "print(\"Single score:\", round(single_silhouette, 4))\n",
    "print(\"Complete score:\", round(complete_silhouette, 4))\n",
    "print(\"'True' score:\", round(true_silhouette, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the difference in silhouette score that the complete linkage method performs much better than the single linkage. In fact, compared to the best possible silhouette score (using the true labels) the complete linkage performs *extremely* well in this toy example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12,5), ncols=2)\n",
    "\n",
    "axes[0].set_title(\"Single Linkage\")\n",
    "axes[0].scatter(x=X[:,0], y=X[:,1], c=single_clusters, cmap=\"viridis\")\n",
    "\n",
    "axes[1].set_title(\"Complete Linkage\")\n",
    "axes[1].scatter(x=X[:,0], y=X[:,1], c=complete_clusters, cmap=\"viridis\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure it is clear that the single-linkage has grouped two of the large clusters together, and kept a single point as a cluster. This has happened because one single point is further away from any other cluster than any of the points within any other cluster.\n",
    "\n",
    "In this sense, the single-linkage can be very susceptible to statistical noise, and cannot reliable give us the cluster groupings we want. \n",
    "\n",
    "Where our data is compact, the complete linkage can perform very well, such as in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "Why might agglomerative clustering be slow / resource demanding to impliment?\n",
    "\n",
    "---\n",
    "\n",
    "In a naive implementation of the hierarchical clustering described, the time taken to complete the modelling will increase significantly as more data is added.\n",
    "\n",
    "When the algorithm starts, the distance between every single data point needs to be calculated to every other data point. If we have $N$ data points that is $N^2$ distance calculations just for the first step. \n",
    "\n",
    "For a few hundred/thousand data points each with a couple dimensions this is easy for our computers. However, as we increase the dimensionality of our data (columns), and the number of points (rows), the number of calculations increases rapidly. \n",
    "\n",
    "Thankfully, most clustering algorithms do not need to calculate every single step and can take some shortcuts to bypass some computing effort. The more shortcuts we make however, the higher the potential for less accurate results. \n",
    "\n",
    "As a result of the number of calculations, some algorithms can become slow or impossible to run on large data. \n",
    "\n",
    "For hierarchical clustering we need to store information about the clustering at each step as each merge occurs, this can take up a large amount of computer memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "Using the \"three_clusters_data\" imported below, cluster the data with $n\\_clusters=3$ use single linkage and the *scipy* package. \n",
    "\n",
    "Calculate the silhouette score of this clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_clusters_data = np.loadtxt(\"../data/clusters.csv\", delimiter=\",\")\n",
    "X, labels_true = three_clusters_data[:,0:-1], three_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "# Calculate the linkage matrix\n",
    "single_linkage_matrix = linkage(X, 'single')\n",
    "\n",
    "# Flatten the linakge matrix to the required number of clusters\n",
    "found_single_labels = fcluster(single_linkage_matrix,\n",
    "                               criterion=\"maxclust\", t=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_score = silhouette_score(X, found_single_labels)\n",
    "print(\"Single Linkage silhouette score\", round(single_score, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "Plot a dendrogram using the data loaded below, previously used in exercise one, and Ward linkage. \n",
    "\n",
    "Looking at the dendrogram, what appears to be a sensible number of clusters to take?\n",
    "\n",
    "Calculate the silhouette value of this number of clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and separate data\n",
    "unknown_clusters_data = np.loadtxt(\"../data/exercise_one_clusters.csv\", delimiter=\",\")\n",
    "X, _true_labels= unknown_clusters_data[:,0:-1], unknown_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "# Calculate the linkage matrix\n",
    "ward_linkage_matrix = linkage(X, 'ward')\n",
    "\n",
    "\n",
    "# Create the dendrogram object, show the counts and specify the method of truncation\n",
    "average_dendrogram = dendrogram(ward_linkage_matrix, \n",
    "                                show_leaf_counts=True, \n",
    "                                p=10, \n",
    "                                truncate_mode=\"lastp\")\n",
    "plt.title(\"Dendrogram of Ward Linkage\")\n",
    "plt.ylabel(\"Merge Distance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dendrogram we can see that there is a significant amount of distance between the merging of the final two clusters. There too are significant gaps between merges down until there are 5 clusters. After 5 clusters the distance to join successive clusters decreases. This implies that 5 clusters may be a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the linkage matrix to the required number of clusters\n",
    "found_ward_labels = fcluster(ward_linkage_matrix,\n",
    "                               criterion=\"maxclust\", t=5)\n",
    "\n",
    "# Calculate the performance\n",
    "ward_score = silhouette_score(X, found_ward_labels)\n",
    "\n",
    "print(\"Ward silhouette score:\", round(ward_score, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number of clusters appears to perform well, but why not check above and below to make sure well didn't misinterpret the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through a range of number of clusters\n",
    "for number_of_clusters in range(2, 9):\n",
    "    # Flatten the linkage matrix to the specified number of clusters\n",
    "    found_ward_labels = fcluster(ward_linkage_matrix,\n",
    "                                 criterion=\"maxclust\", \n",
    "                                 t=number_of_clusters)\n",
    "\n",
    "    ward_score = silhouette_score(X, found_ward_labels)\n",
    "    print(\"{} clusters, score: {:.4}\".format(number_of_clusters, ward_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with regards to our evaluation method, 5 clusters is the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
