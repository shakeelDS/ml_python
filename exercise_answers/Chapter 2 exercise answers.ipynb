{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
=======
   "metadata": {},
>>>>>>> KJ
   "outputs": [],
   "source": [
    "# Import relavent libraries\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import data\n",
    "bikes_filepath = '../data/bikes.csv'\n",
    "data = pd.read_csv(filepath_or_buffer=bikes_filepath, delimiter=\",\")\n",
    "\n",
    "# Clean missing data.\n",
    "clean_data = data.interpolate(method='linear')\n",
    "clean_data = clean_data.fillna(method=\"bfill\")\n",
    "\n",
    "# Set our target variable as the count.\n",
    "y = clean_data[\"count\"].to_numpy()\n",
    "\n",
    "# Initialise scaler\n",
    "rb_scaler = RobustScaler()\n",
    "\n",
    "# We are just going to use numerical data to begin with\n",
    "# dropping our count and multicolinear feel_temp.\n",
    "numerical_data = clean_data.select_dtypes(include='number')\n",
    "numerical_data = numerical_data.drop(columns=[\"count\", \"feel_temperature\"])\n",
    "\n",
    "# Scale our data\n",
    "scaled_features = rb_scaler.fit_transform(numerical_data)\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=numerical_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Using all of the columns from the scaled data.\n",
    "X = scaled_data.to_numpy()\n",
    "\n",
    "# Split the training and test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Initialise model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit model\n",
    "linear_model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from data\n",
    "y_pred = linear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R^2 value using the true and predicted values of y test.\n",
    "r2_value = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Three Attributes R2 value: \\n\", r2_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial $R^2$ value with `random_state=1234` was 59%.\n",
    "Our new $R^2$ value with `random_state=0` is 65%. \n",
    "This shows that the value of the $R^2$ value is very dependent on the random split between our training and test data. Some splits will make our model able to explain the variance in the test data well, some may be over fit to the training data. Our score depends on the split we get. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialise model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Generate cross-val scores using model and data\n",
    "cv_scores_K2 = cross_val_score(estimator=linear_model, X=X, y=y, cv=2, scoring=\"r2\")\n",
    "cv_scores_K10 = cross_val_score(estimator=linear_model, X=X, y=y, cv=10, scoring=\"r2\")\n",
    "\n",
    "\n",
    "print(\"K=2 scores: \\n\", cv_scores_K2, \"\\nK=2 average R2: \\n\", cv_scores_K2.mean())\n",
    "print(\"\\nK=10 scores: \\n\", cv_scores_K10, \"\\nK=10 average R2: \\n\", cv_scores_K10.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clearly that the number of folds in our model can affect how our evaluation performs. With a test set of only 10% of the data (K=10) we are not able to accurately predict the variance in our model consistently. With higher K each evaluation is dependent more on the individual split. For K=2 we can get generally good evaluation, showing that our model even with less training data can explain the variance, this implies our model is well generalised and not overfit for lower K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Initialise the regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data.\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions using the test set.\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Calculate the MSE between the true and predicted values.\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "print(\"Model MSE: \\n\", MSE)\n",
    "print(\"Model RMSE: \\n\", RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "scrolled": true
   },
=======
   "metadata": {},
>>>>>>> KJ
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Set a value for alpha.\n",
    "alpha_value = 9\n",
    "\n",
    "# Initialise the model\n",
    "ridge_model = Ridge(alpha=alpha_value)\n",
    "\n",
    "# Keeping the -1 in to ensure positive values, generate the cv_scores using the model.\n",
    "cv_scores = -1*cross_val_score(estimator=ridge_model, X=X, y=y, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "\n",
    "print(\"Value of α:\", alpha_value)\n",
    "print(\"K-fold MAE values: \\n\", cv_scores)\n",
    "print(\"Mean value across K-folds: \\n\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the naive approach of plugging in numbers the best integer value of $\\alpha$ was 9, can you get a better average MAE than this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Set a value for alpha.\n",
    "alpha_value = 1\n",
    "\n",
    "lasso_model = Lasso(alpha=alpha_value)\n",
    "\n",
    "# split the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# train model\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# predict values\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "# get score\n",
    "MAE_value = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Value of α:\", alpha_value)\n",
    "print(\"MAE score: \\n\", MAE_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MAE score for lasso is broadly similar at least for the values of $\\alpha$ I have chosen. However, it is bad practice to compare the score of one split to the average of another, because our one split is highly dependent on the random split chosen. We want to compare cross validated scores with other cross validated scores using the same number of folds. \n",
    "\n",
    "Instead of changing the values of $\\alpha$ by hand and looking at the change in output score we could generate a range of values and evaluate them all, selecting the highest performing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
