{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
=======
   "metadata": {},
>>>>>>> KJ
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load data\n",
    "income_filepath = '../data/income.csv'\n",
    "income_data = pd.read_csv(filepath_or_buffer=income_filepath, delimiter=\",\")\n",
    "\n",
    "# Drop missing data and reset indexes\n",
    "income_data = income_data.dropna()\n",
    "income_data = income_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
=======
   "metadata": {},
>>>>>>> KJ
   "outputs": [],
   "source": [
    "# Density plot of Age attribute.\n",
    "income_data[\"age\"].plot.density()\n",
    "plt.xlabel(\"Age\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Education years distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
=======
   "metadata": {},
>>>>>>> KJ
   "outputs": [],
   "source": [
    "# Density plot of education years attribute.\n",
    "income_data[\"education_years\"].plot.density()\n",
    "plt.xlabel(\"Years in education\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work hours distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot of work hours distribution.\n",
    "income_data[\"work_hours\"].plot.density()\n",
    "plt.xlabel(\"Work hours\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that both the **`work_hours`** and **`education_years`** are clearly not normally distributed. A standard scaler would not be useful here. The **`age`** attribute, while more symetric and having an even spread, still is somewhat skewed. The **`work_hours`** has some significantly large outliers, with some people working 100 hours while most are around 40. For these reasons - the non-normal distribution and outliers, I have chosen to use the robust scaler for all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessing libraries\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, RobustScaler\n",
    "\n",
    "# Initialise one hot encoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "rb_scaler = RobustScaler()\n",
    "\n",
    "# Set our target variable to be a separate array.\n",
    "target_data = pd.DataFrame(income_data[\"salary_over_50K\"])\n",
    "y = target_data[\"salary_over_50K\"].to_numpy()\n",
    "\n",
    "# Remove target variable in order to get predictor data.\n",
    "income_data = income_data.drop(columns=[\"salary_over_50K\"])\n",
    "\n",
    "# Separate the numerical data and categorical data.\n",
    "X_num = income_data.select_dtypes(exclude=\"object\").to_numpy()\n",
    "X_cat = income_data.select_dtypes(include=\"object\").to_numpy()\n",
    "\n",
    "# Fit the one hot encoder to the categorical data.\n",
    "ohe.fit(X_cat)\n",
    "\n",
    "# Transform the categorical data frame and convert it to an array\n",
    "X_cat = ohe.transform(X_cat).toarray()\n",
    "\n",
    "# Fit the scaler to the numeric data\n",
    "rb_scaler.fit(X_num)\n",
    "\n",
    "# Transform the raw numerical data to a scaled version.\n",
    "X_num = rb_scaler.transform(X_num)\n",
    "\n",
    "# Combine the categorical and numerical data back to a single array.\n",
    "X = np.concatenate((X_num, X_cat), axis=1)\n",
    "\n",
    "# Initialise the encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the target data\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Initialise model\n",
    "stratified_model = DummyClassifier(strategy=\"stratified\")\n",
    "\n",
    "# Train the model\n",
    "stratified_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict values for testing data\n",
    "y_pred = stratified_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of predictions\n",
    "stratified_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model accuracy: \\n\", stratified_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "We can see that the number of folds in our model can affect how our evaluation performs. With a test set of only 10% of the data (K=10) we are not able to accurately predict the variance in our model consistently. With higher K each evaluation is dependent more on the individual split. For K=2 we can get generally good evaluation, showing that our model even with less training data can explain the variance, this implies our model is well generalised and not overfit for lower K."
=======
    "Our stratified model, which takes each class into account equally, produces an approximate accuracy of 50% (try run it a few times without a random state argument!). This shows us a different result to the most frequent classifier, which is affected greatly by the class distribution. The stratified dummy model isn't a useful model in practice, but it does nicely illustrate how stratification can be used to avoid class distribution bias in our models. "
>>>>>>> KJ
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialise object\n",
    "most_frequent_model = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "# Train the model\n",
    "most_frequent_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict from the testing data\n",
    "y_pred_most_frequent = most_frequent_model.predict(X_test)\n",
    "\n",
    "# Set our labels for the matrix and plot, ensure they correspond ie 0->\"No\"\n",
    "labels = [0,1]\n",
    "tick_labels = [\"No\", \"Yes\"]\n",
    "\n",
    "# Generate confusion matrix from true and predicted values.\n",
    "most_frequent_conf_matrix = confusion_matrix(y_test, y_pred_most_frequent, labels=labels, normalize=\"true\")\n",
    "\n",
    "print(\"Confusion matrix (proportion true): \\n\", most_frequent_conf_matrix)\n",
    "\n",
    "# Plotting the confusion matrix with color scale\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(most_frequent_conf_matrix, cmap=\"coolwarm\")\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + tick_labels)\n",
    "ax.set_yticklabels([''] + tick_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion matrix is very different from our logistic regression one. We can clearly see what we discussed about the most frequent model, it predicts every single instance of the data as \"No\" whereas the LR model predicts either \"Yes\" or \"No\" with some likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the names for our report to produce.\n",
    "target_names = [\"No\", \"Yes\"]\n",
    "\n",
    "# Generate the report using the target test and prediction values.\n",
    "classif_report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "print(classif_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b><font size=3> UndefinedMetricWarning<font> </b> \n",
    "<p> \n",
    "The UndefinedMetricWarning produced isn't anything to worry about and is a result of the nature of our model, we won't try to get rid of it as it shows our point! This warning is produced when we evaluate a model that hasn't predicted values we have\n",
    "said are possible predictions.\n",
    "\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from our classification report the importance of the different metrics we have shown. The \"No\" prediction has a 1.00 recall, but a 0.60 precision, showing that using only one or the other would be misleading. The \"Yes\" scores zero across the board. In the logistic regression case there was more of a mix in the recall and precision values, with actual values for the \"Yes\" prediction.\n",
    "\n",
    "This also shows the difference between the macro average and weighted average. The macro takes an unweighted mean between the different label's f1 scores. \n",
    "> $$macro~avg = \\frac{0.74+0.00}{2} = 0.37$$ \n",
    "\n",
    "This is interesting, but remember we have an uneven class distribution, so this could be a misleading interpretation of our results.\n",
    "\n",
    "The weighted (or micro) average takes into account the number of each label in the test set. This is called weighting by support per label. In our example it is:\n",
    "> $$weighted~avg = \\frac{1110}{1864}\\times0.74 + \\frac{754}{1864}\\times0.00 = 0.43$$\n",
    "\n",
    "This is a better way in our example of evaluating the performance of our unevenly class distributed test set. This becomes even more crucial when we have multiple labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# We need two different sets of data, one to train our model and one to evaluate.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Define the parameters and the values we want to search.\n",
    "parameters_to_search = {\"tol\":[0.000000001, 0.00000001, 0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# Select the model type we have chosen.\n",
    "logistic_regression_model = LogisticRegression(solver=\"liblinear\", C=1, class_weight=\"balanced\")\n",
    "\n",
    "# Set the number of folds we want to have.\n",
    "K = 5\n",
    "\n",
    "# Define our grid to find optimal model.\n",
    "optimised_model = GridSearchCV(estimator=logistic_regression_model, param_grid=parameters_to_search, scoring=\"f1\", cv=K)\n",
    "\n",
    "# Fit our parameter search model.\n",
    "optimised_model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best parameters found are: \\n\", optimised_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value for **`tol`** predicted is not the smallest one possible, implying that having a more granular solver does not necessarily yield the best value, but can perform better even with larger tolerances than default. The differences in performance can be tested. Do you think they are significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b><font size=3> UndefinedMetricWarning<font> </b> \n",
    "<p> \n",
    "This warning is produced when a model is evaluated and there are no examples of one of the classes in the predicted data. In our example, this is because for high values of *\"tol\"* the model is very bad as it stops finding a solution too early. This means it does not predict one of the values, which causes the warning.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
