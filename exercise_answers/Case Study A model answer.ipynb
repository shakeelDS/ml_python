{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set file path\n",
    "titanic_filepath = '../data/titanic.csv'\n",
    "\n",
    "# Import data into a data frame.\n",
    "raw_data = pd.read_csv(filepath_or_buffer=titanic_filepath, delimiter=\",\")\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get information about the data types.\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "raw_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age data is the only one with any missing data. Will age be a significant determiner of the \"Survived\" class? Possibly, so it shouldn't necessarily be dropped. The missing data accounts for approximately 20% of the rows, which means it could have a significant impact on our model performance. We may want to revist how we cleaned this data later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As a first approximation, will repalce the missing data with the mean value\n",
    "\n",
    "clean_data = raw_data.fillna(raw_data[\"Age\"].median())\n",
    "\n",
    "# Our plot shows a somewhat symetrically distributed distribution.\n",
    "clean_data[\"Age\"].plot.density();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the name and passenger ID's are going to be unique to each row, and therefore need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove unique columns.\n",
    "basic_feature_data = clean_data.drop(columns=[\"Name\", \"PassengerId\"])\n",
    "basic_feature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the target class distribution of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the amount of each target class is within the data.\n",
    "basic_feature_data[\"Survived\"].value_counts(normalize=True).plot(kind=\"bar\",\n",
    "                                                    color=[\"navy\", \"gold\"],\n",
    "                                                    title=\"Survival class distribution\");\n",
    "\n",
    "print(basic_feature_data[\"Survived\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution may reduce the performance of our model, we are going to resample our data so that we have the same amount for both the classes. This will be achieved by undersampling the majority class (\"No\"/0), we have a reasonable number of minority class (342 samples) and therefore although we will lose some predictive power for \"No\", our model will generalise better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = basic_feature_data[basic_feature_data[\"Survived\"]==0]\n",
    "df_minority = basic_feature_data[basic_feature_data[\"Survived\"]==1]\n",
    " \n",
    "# Undersample majority class.\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=342,    # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "\n",
    "basic_feature_data = pd.concat([df_majority_downsampled, df_minority], axis=0, sort=True)\n",
    "\n",
    "basic_feature_data = basic_feature_data.reset_index(drop=True)\n",
    "\n",
    "# Display new class counts\n",
    "basic_feature_data[\"Survived\"].value_counts()\n",
    "# 1    576\n",
    "# 0    576\n",
    "# Name: balance, dtype: int64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our \"Sex\" attribute is categorical, and should be encoded to numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise the encoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "# Make array of encoded sex column\n",
    "sex_data_array = one_hot_encoder.fit_transform(basic_feature_data[[\"Sex\"]]).toarray()\n",
    "\n",
    "# Store the different categories\n",
    "column_names = one_hot_encoder.get_feature_names(['Sex'])\n",
    "\n",
    "# Create a new data frame with the sex data.\n",
    "sex_encoded_data = pd.DataFrame(data=sex_data_array, columns=column_names)\n",
    "\n",
    "sex_encoded_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets separate our target variable from our features and add our new \"Sex\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign target to a separate object.\n",
    "survived_labels = basic_feature_data[\"Survived\"]\n",
    "\n",
    "# Remove the original categorical data column and the target.\n",
    "basic_feature_data = basic_feature_data.drop(columns=[\"Survived\", \"Sex\"])\n",
    "\n",
    "# Combine the numerical and encoded data into one frame.\n",
    "basic_feature_data = pd.concat([basic_feature_data, sex_encoded_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to scale our data. Because we are using a model which uses a distance metric it is important to use normalisation scaling so that all features are comparable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise normalizer\n",
    "normal_scaler = Normalizer()\n",
    "\n",
    "# Fit and transform the data with the normalizer.\n",
    "normalised_feature_data = pd.DataFrame(normal_scaler.fit_transform(X=basic_feature_data), \n",
    "                                       columns=[basic_feature_data.columns])\n",
    "\n",
    "normalised_feature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume that we will get the best performance of our model using all the variables. We now have some feautres ready to use, we need to make a training and test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data set into training and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalised_feature_data.to_numpy(), \n",
    "                                                    survived_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first train and evaluate a model using the default arguments, where $K=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise the classifier object\n",
    "neighbour_initial_model = KNeighborsClassifier()\n",
    "\n",
    "# Fit the model to the training data.\n",
    "neighbour_initial_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict values on the test set using the trained model.\n",
    "init_y_pred = neighbour_initial_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the names for the classification report to produce.\n",
    "target_names = [\"No\", \"Yes\"]\n",
    "\n",
    "# Generate the report using the target test and prediction values.\n",
    "classif_report = classification_report(y_test, init_y_pred, target_names=target_names)\n",
    "\n",
    "print(classif_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good first attempt, but we could improve the F1 score probably by selecting a better K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters and the values we want to search.\n",
    "parameters = {\"n_neighbors\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}\n",
    "\n",
    "# Select the model type we have chosen.\n",
    "neighbour_improved_model = KNeighborsClassifier()\n",
    "\n",
    "# Set the number of folds we want to have to get 80:20 train/test split.\n",
    "n_cv = 5\n",
    "\n",
    "# Define our grid search model to find optimal parameters.\n",
    "opt_model = GridSearchCV(estimator=neighbour_improved_model, param_grid=parameters, scoring=\"f1\", cv=n_cv)\n",
    "\n",
    "# Fit our parameter search model.\n",
    "opt_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nThe best parameters found are: \\n\\n\", opt_model.best_params_)\n",
    "\n",
    "# Predict target values based on best model found.\n",
    "better_y_pred = opt_model.best_estimator_.predict(X_test)\n",
    "\n",
    "# Generate the report using the target test and predicted values.\n",
    "classif_report_new = classification_report(y_test, better_y_pred, target_names=target_names)\n",
    "\n",
    "print(classif_report_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have managed to increase the F1 score by 4% by just changing $K$, but the nearer we get to 100% the harder it is to increase our score as we get closer to the limit of reducible error. Maybe a more flexible model would be useful here or there are better ways to prepare our data. How would you improve this workflow? What is the highest score you can achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
