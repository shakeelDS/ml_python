{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction answers\n",
    "\n",
    "Notebook containing the example answers to Chapter 4 - Dimension reduction.\n",
    "\n",
    "Note: these answers are examples and your way of tackling the problem may also be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# function from original notebook\n",
    "from dim_red_function import PCA_train_predict_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dim = np.loadtxt(\"../data/high_dimensions.csv\", delimiter=\",\")\n",
    "\n",
    "# Splitting the data set\n",
    "X = high_dim[:,0:-1]\n",
    "y = high_dim[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Perform PCA again on $X$, this time with $n\\_components=10$, what is the difference in **total** explained variance between using five and ten components? Which number of components might we prefer to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the decomposition of the data\n",
    "pca_5 = PCA(n_components=5).fit(X)\n",
    "pca_10 = PCA(n_components=10).fit(X)\n",
    "\n",
    "# Calculate total variance ratio\n",
    "tevr_5 = pca_5.explained_variance_ratio_.sum()\n",
    "tevr_10 = pca_10.explained_variance_ratio_.sum()\n",
    "\n",
    "print(\"5 components: \", tevr_5)\n",
    "print(\"10 components: \", tevr_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between the ratios (largest - smallest)\n",
    "tevr_difference = tevr_10 - tevr_5\n",
    "\n",
    "print(\"Difference in total explained ratio: \", round(tevr_difference, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of $k = 10$ explains more of the variance of the original data, so we would prefer to use that rather than $k = 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "What value of $k$ has the highest associated F1 score? What does that tell you about our data?\n",
    "\n",
    "**HINT** Look up the numpy function $np.argmax()$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of components is given by the original feature number\n",
    "k_max = X.shape[1]\n",
    "\n",
    "# Create a list of k_values\n",
    "k_values = list(range(1, k_max))\n",
    "\n",
    "# Generate F1 scores for each k value of PCA\n",
    "f1_scores = [PCA_train_predict_score(X, y, k) for k in k_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the index of the highest k values here\n",
    "highest_value_index = np.argmax(f1_scores)\n",
    "\n",
    "# The k value this corresponds to is\n",
    "highest_k = k_values[highest_value_index]\n",
    "\n",
    "print(\"The value of K with the highest F1 score is:\", highest_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Using $X$ fit a PCA model with $k = 50$.\n",
    "\n",
    "Plot the cumulative sum of variance explained ratio of each component.\n",
    "\n",
    "**HINT:** use the numpy $np.cumsum()$ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object with the decomposed data\n",
    "pca_50 = PCA(n_components=50).fit(X)\n",
    "\n",
    "# Calculate the cumulative sum of each component evr\n",
    "cumulative_sum_ver = np.cumsum(pca_50.explained_variance_ratio_)\n",
    "\n",
    "# Create a list of the component numbers\n",
    "components_values = list(range(1, 51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Cumulative explained variance ratio at number of components\")\n",
    "plt.xlabel(\"number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance ratio\")\n",
    "plt.plot(components_values, cumulative_sum_ver, c=\"navy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the explained variance ratio increases sharply at low numbers of components. Around 5-10 components this rate decreases, showing that each additional component explains less additional variance than the earlier components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Using $X\\_circles$ perform *linear* PCA with $k = 2$. Plot the resulting new dimensions of data. Calculate the **total** explained variance ratio of the model and discuss why it may be difficult for a linear machine learning model to learn the pattern of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our concentric circle data\n",
    "circle_data = np.loadtxt(\"../data/circles.csv\", delimiter=\",\")\n",
    "\n",
    "# Split X and Y data\n",
    "X_circles, y_circles = circle_data[:,0:-1], circle_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the PCA object\n",
    "pca_circles_2 = PCA(n_components=2).fit(X_circles)\n",
    "\n",
    "# Generate the PCA'd data\n",
    "X_circles_2 = pca_circles_2.transform(X_circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(\"Linear PCA Dimensions of Features\")\n",
    "plt.xlabel(\"$component~1$\")\n",
    "plt.ylabel(\"$component~2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.scatter(x=X_circles_2[:,0], y=X_circles_2[:,1], c=y_circles, cmap=\"coolwarm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this view of our data shows that we still have two classes, one a circle within the ring of another. If we consider a simple linear model that will help us determine which class is which it is difficult to separate the classes. There is no line that you can draw in this space that will have one class mostly on one side and the other class mostly on the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Perform Kernel PCA on the $X\\_circles$ data set using the \"rbf\" kernel with $k = 2$.\n",
    "\n",
    "Use values of $gamma=1$ and $gamma=7$. What effect does this have?\n",
    "\n",
    "Plot the results of each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kernel PCA objects with associated parameters\n",
    "kpca_g_1 = KernelPCA(n_components=2, kernel=\"rbf\", gamma=1)\n",
    "kpca_g_7 = KernelPCA(n_components=2, kernel=\"rbf\", gamma=7)\n",
    "\n",
    "# Generate the data from Kernel PCA with each parameter\n",
    "X_g_1 = kpca_g_1.fit_transform(X_circles)\n",
    "X_g_7 = kpca_g_7.fit_transform(X_circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=True, sharex=True, figsize=(10, 5))\n",
    "\n",
    "ax1.set_title(\"rbf kernel $gamma=1$\")\n",
    "ax1.set_xlabel(\"$component~1$\")\n",
    "ax1.set_ylabel(\"$component~2$\")\n",
    "ax1.scatter(x=X_g_1[:,0], y=X_g_1[:,1], c=y_circles, cmap=\"coolwarm\");\n",
    "\n",
    "ax2.set_title(\"rbf kernel $gamma=7$\")\n",
    "ax2.set_xlabel(\"$component~1$\")\n",
    "ax2.set_ylabel(\"$component~2$\")\n",
    "ax2.scatter(x=X_g_7[:,0], y=X_g_7[:,1], c=y_circles, cmap=\"coolwarm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of `gamma` determines the strength of the radial basis function kernel effect. The larger `gamma` transforms the data more using the radial coordinates (distance from centre) which gives a more pronounced change to our circlular data. `gamma` is a hyperparameter which means we can tune it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "Using $ipca$ fit the data to $X$. Compare the total variance explained ratio with the original PCA method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit incremental PCA object\n",
    "ipca_20 = IncrementalPCA(n_components=20).fit(X)\n",
    "\n",
    "# Create and fit linear PCA object\n",
    "pca_20 = PCA(n_components=20).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca_20_tevr = ipca_20.explained_variance_ratio_.sum()\n",
    "pca_20_tevr = pca_20.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IPCA total explained variance ratio: {}%\".format(round(100*ipca_20_tevr, 2)))\n",
    "print(\"PCA total explained variance ratio: {}%\".format(round(100*pca_20_tevr, 2)))\n",
    "print(\"Difference: {}%\".format(round(100*(pca_20_tevr - ipca_20_tevr), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a difference in performance between the two methods for the data and number of components given. However, this difference is small in scale, the difference in model performance as a result will be small. The added benefit of being able to use this method for data which will not fit into memory can outway the slight performance reduction in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "Using /data/bikes.csv, remove the non-numeric data, visualise the data in 2D using t-SNE and the target \"count\" attribute.\n",
    "\n",
    "Compare t-SNE and PCA in 2D for this data. \n",
    "\n",
    "What value of $k$ is needed have the total variance explained ratio $> 0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_raw_data = pd.read_csv(\"../data/bikes.csv\")\n",
    "\n",
    "# Keep only boolean and numerical data in the frame\n",
    "bikes_numeric = bikes_raw_data.select_dtypes(include=[\"float64\", \"int64\", \"bool\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop our missing data for each\n",
    "bikes_clean = bikes_numeric.dropna()\n",
    "\n",
    "# Convert to numpy\n",
    "bikes = bikes_clean.to_numpy()\n",
    "\n",
    "# Separate features and target\n",
    "X_bikes, y_bikes = bikes[:,:-1], bikes[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dimension reduction objects\n",
    "tsne_2 = TSNE(n_components=2)\n",
    "pca_2 = PCA(n_components=2)\n",
    "\n",
    "# Generate the new data for each technique\n",
    "X_bikes_tsne = tsne_2.fit_transform(X_bikes)\n",
    "X_bikes_pca = pca_2.fit_transform(X_bikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "ax1.set_title(\"t-SNE\")\n",
    "ax1.set_xlabel(\"$component~1$\")\n",
    "ax1.set_ylabel(\"$component~2$\")\n",
    "tsne_plot = ax1.scatter(x=X_bikes_tsne[:,0], y=X_bikes_tsne[:,1], c=y_bikes, cmap=\"coolwarm\")\n",
    "fig.colorbar(tsne_plot, ax=ax1)\n",
    "\n",
    "ax2.set_title(\"PCA\")\n",
    "ax2.set_xlabel(\"$component~1$\")\n",
    "ax2.set_ylabel(\"$component~2$\")\n",
    "pca_plot = ax2.scatter(x=X_bikes_pca[:,0], y=X_bikes_pca[:,1], c=y_bikes, cmap=\"coolwarm\")\n",
    "fig.colorbar(pca_plot, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the two different methods are giving us significantly different structures of our data with regards to the target class. Both seem to separate the values of \"count\" to some extent, with the t-SNE creating a clear separate boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit object with all components (default number)\n",
    "pca_bikes_all = PCA().fit(X_bikes)\n",
    "\n",
    "# Calculate the cumulative sum of each component's ratio in order\n",
    "pca_bikes_all.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From inspection of the cumulative sum of total explained variance we can see that $k=2$ gives us $>0.8$ explained. For larger numbers of components however we can pass the variance we want to achieve to the PCA object instead of the number of components which will then find the right number of components for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify the desired ratio\n",
    "pca_bikes_var = PCA(n_components=0.8).fit(X_bikes)\n",
    "\n",
    "# The PCA object has useful attributes\n",
    "pca_bikes_var.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More explicitly it follows a method similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of the cumulative ratio values\n",
    "cum_sums = pca_bikes_all.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Create a range of k values from 1 to k\n",
    "k_values = range(1, pca_bikes_all.n_features_ + 1)\n",
    "\n",
    "# Loop over the cumulative sums and stop when value > 0.8\n",
    "for index, each_sum in enumerate(cum_sums):\n",
    "    if each_sum > 0.8:\n",
    "        print(\"Number of required k for >= 0.8 explained variance ratio:\", k_values[index])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
