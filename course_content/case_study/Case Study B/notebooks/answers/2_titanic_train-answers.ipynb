{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://datasciencecampus.ons.gov.uk/wp-content/uploads/sites/10/2017/03/data-science-campus-logo-new.svg\"\n",
    "             alt=\"ONS Data Science Campus Logo\"\n",
    "             width = \"240\"\n",
    "             style=\"margin: 0px 60px\"\n",
    "             />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Model Selection\n",
    "\n",
    "purpose of script: compare logreg vs knn on titanic_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cached data from titanic_EDA.py\n",
    "titanic_train = pd.read_pickle('../../cache/titanic_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to preprocess target & features\n",
    "\n",
    "def preprocess_target(df) :\n",
    "    # Create arrays for the features and the target variable\n",
    "    target = df['Survived'].values\n",
    "    return(target)\n",
    "\n",
    "def preprocess_features(df) :\n",
    "    #extract features series\n",
    "    features = df.drop('Survived', axis=1)\n",
    "    #remove features that cannot be converted to float: name, ticket & cabin\n",
    "    features = features.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "    # dummy encoding of any remaining categorical data\n",
    "    features = pd.get_dummies(features, drop_first=True)\n",
    "    # ensure np.nan used to replace missing values\n",
    "    features.replace('nan', np.nan, inplace=True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to use pipeline to select from best model & best parameters. Use the following workflow:\n",
    "\n",
    "* Train-Test-Split\n",
    "* Instantiate\n",
    "* Fit\n",
    "* Predict\n",
    "\n",
    "\n",
    "Prep data for logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess target from titanic_train\n",
    "target = preprocess_target(titanic_train)\n",
    "#preprocess features from titanic_train\n",
    "features = preprocess_features(titanic_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X == features. y == target. Use 25% of data as 'hold out' data. Use a random state of 36.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=36)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute median for NaNs in age column\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "# instantiate classifier\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "steps = [\n",
    "    # impute medians on NaNs\n",
    "    ('imputation', imp),\n",
    "    # scale features\n",
    "    ('scaler', StandardScaler()),\n",
    "    # fit logreg classifier\n",
    "    ('logistic_regression', logreg)]\n",
    "\n",
    "# establish pipeline\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Review performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a classification report of the model's performance passing the true labels and the predicted labels\n",
    "# as arguments in that order\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is 9% lower in the survived category. High precision == low FP \n",
    "rate. This model performs 9 % better in relation to false positives \n",
    "(assigning survived when in fact died) when class assigned is 0 than 1.\n",
    "\n",
    "Recall (false negative rate - assigning died but in truth survived) is 5%\n",
    "higher in 0 class.\n",
    "\n",
    "The harmonic mean of precision and recall - f1 - has a 7 percent increase \n",
    "when assigning 0 as survived. \n",
    "\n",
    "This has resulted in 133 rows (versus 90 rows in survived) of the true\n",
    "response sampled faling within the 0 (died) category.\n",
    "\n",
    "Overall, it appears that this model is considerably better at predicting when\n",
    "people died rather than survived.\n",
    "\n",
    "## Receiver Operator Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the probability of a sample being in a particular class\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:,1]\n",
    "# calculate the false positive rate, true positive rate and thresholds using roc_curve()\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "# create the axes\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# control the aesthetics\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "# x axis label\n",
    "plt.xlabel('False Positive Rate')\n",
    "# y axis label\n",
    "plt.ylabel('True Positive Rate')\n",
    "# main title\n",
    "plt.title('Logistic Regression ROC Curve (titanic_train)')\n",
    "# show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To my eye it looks as though a p value of around 0.8 is going to be optimum. This curve can then be used against further ROC curves to visually compare performance. What is the area under curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AUC of 0.5 is as good as a model randomly asisgning classes and being correct on average 50% of the time. Here we have an untuned AUC of 0.879 which can be used to compare against further models. Precision recall curve not pursued as class imbalance is not high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy up\n",
    "del fpr, logreg, pipeline, steps, thresholds, tpr, y_pred, y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## KNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    # impute median for any NaNs\n",
    "    ('imputation', imp),\n",
    "    # scale features\n",
    "    ('scaler', StandardScaler()),\n",
    "    # specify the knn classifier function for the 'knn' tep below, specifying k=5\n",
    "    ('knn', KNeighborsClassifier(5))]\n",
    "\n",
    "# establish a pipeline for the above steps\n",
    "pipeline = Pipeline(steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test features using the KNN pipeline you have created\n",
    "y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Review performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above, calculate accuracy of the classifier, but this time on the test data\n",
    "pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positive marginally higher than in logreg and true negaive identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is still lower within the survived category, however the difference\n",
    "has now reduced from 9 % to 8 % lower than the logreg model. Averages are up\n",
    "marginally over logreg.\n",
    "\n",
    "Recall (false negative rate - assigning died but in truth survived) within the\n",
    "survived predicted group has increased by 2 % than in logreg.\n",
    "\n",
    " harmonic mean of these - f1 is similarly reduced. f1 has been marginally \n",
    " improved over logreg in both classes and average.\n",
    "\n",
    "support output has been unaffected. \n",
    "\n",
    "KNN appears to marginally outperform logreg. Now need to compare whether \n",
    "titanic_engineered adds any value.\n",
    "\n",
    "## Receiver Operator Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the probability of a sample being in a particular class\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:,1]\n",
    "# unpack roc curve objects\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "# plot an ROC curve using matplotlib below:\n",
    "# create axes\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# control aesthetics\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC Curve (titanic_train)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve looks very similar to the logreg model, I imagine AUC will be very similar also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AUC of 0.5 is as good as a model randomly assigning classes and being correct on average 50% of the time. Here we have an untuned AUC of 0.872 This is down on the logreg AUC by 0.007."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
