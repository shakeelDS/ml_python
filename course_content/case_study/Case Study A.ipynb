{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1><font size=7> Case Study A</font> </h1> </center>\n",
    "\n",
    "This task will give you hands on experience of solving a machine learning problem from start to finish. In this notebook you will be introduced to a new type of model which can be used for both classification and regression purposes. It is based on a powerful algorithm which is frequently used in machine learning. \n",
    "\n",
    "The task will entail a new data set, and you will make choices about how to process the data and build the model using the experience gained in the chapters of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbour Algorithm\n",
    "\n",
    "The principle behind the K-nearest neightbour (KNN) algorithm is that data which have *'similar'* values are likely to have the same class/target value. The KNN uses something briefly discussed earlier called the *'data space'* of features. This is where the data is represented by a vector, where each feature/attribute is a dimension in space. For two features we have a 2D plot, three a 3D plot etc (don't worry you don't have to imagine higher numbers in a real space!).\n",
    "\n",
    "A key feature of the model is the distance calculation, this is used to calculate which data is the *'nearest'* to our new point. The model uses the Euclidean distance to measure how far apart two data points are across the $n$ different dimensions. The distance between data points $\\textbf{p}$ and $\\textbf{q}$ are given as:\n",
    "\n",
    "$$d(\\textbf{p},\\textbf{q}) = \\sqrt{\\sum_{i=1}^{n}(p_i - q_i)^2} $$\n",
    "\n",
    "Where $i$ denotes the $i$'th dimension. An example for calculating the distance between two data points with two features $x$ and $y$ would be:\n",
    "\n",
    "$$d(\\textbf{p},\\textbf{q}) = \\sqrt{(p_x - q_x)^2 + (p_y - q_y)^2} $$\n",
    "\n",
    "What we as machine learning practitioners need to is find an appropriate number for the value $K$. $K$ determines how many neighbours we are going to take into account to determine the label of our new data point. We calculate the distance between all our data points and the new point then select the K-nearest. Our prediction for the new data point is then whichever class the majority of neighbours have. \n",
    "\n",
    "For example, if we have $K=10$ and 7 of the nearest data points are Class A and 3 are Class B our new data point will be predicted as Class A. \n",
    "\n",
    "Below is an illustration of the KNN classification. For our test data point if we take $K = 1$ then our point is classified as Class B. However, this doesn't quite look right as the majority of the data points on that side of the data space are Class A. If we take a larger value, $K = 4$ then the neighbours are majority Class A. This shows that choosing the right value of $K$ is really important when using this model. The best value for $K$ will be a result of properties of the data. We often us a grid search as shown in Chapter 4 to find $K$.\n",
    "\n",
    "<img src=\"../../images/KNN.png\" width=\"600\" height=\"600\" alt=\"Image showing a plot of data points with two different classes approximately separated. The test point is classed according to the nearest neightbour algorithm with K = 1 and K = 4, which give different results.\">\n",
    "\n",
    "For a regression task the KNN regressor will predict the continuous value imputed from the nearest neighbours to the test data point.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b><font size=3> Key Point <font> </b> \n",
    "<p> \n",
    "The optimal value of $K$ is highly data dependent. You should try understand how your classes are distributed with relation to different variables in order to help understand what might be a good scale for $K$. Typically, large values of $K$ help reduce the efect of 'noisy' data. However, this can reduce the model's ability to determine distinctions in boundries in the data. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The KNN classifier is implemented in **`sklearn`** using the class **`sklearn.neighbors.KNeighborsClassifier`**. The argument **`n_neighbors`** is equivalent to $K$ in our description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling with `sklearn`\n",
    "\n",
    "In the Chapter 4 we mentioned different methods of dealing with class imbalances, one of which was to use an argument within the model selected. However, not all models have the `\"class_weight\"` argument associated, we therefore need to re-weight our data manually. \n",
    "\n",
    "Using the `sklearn.utils` resample utlity package allows us to easily change the distribution of our data set. \n",
    "\n",
    "We have two options when resampling our distribution, we can either *downsample* the majority class which reduces the number of the most prevelant class to that of the minority class. On the otherhand we can *upsample* the minority class, where we create new repeated records of the minority class so that there are the same number as the most prevelent class.\n",
    "\n",
    "Below is a brief introduction to the syntax of resampling with `resample`, we will be downsampling. More detail on the function can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html).\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate out the classes into different dataframes\n",
    "majority_class_df = our_dataframe[our_dataframe[\"target_class\"]==0]\n",
    "minority_class_df = our_dataframe[our_dataframe[\"target_class\"]==1]\n",
    "\n",
    "# Find how many are in the minority class\n",
    "number_in_minority_class = len(minority_class_df.index)\n",
    "\n",
    "# Create a new object with the resampled majority class.\n",
    "majority_class_downsampled = resample(majority_class_df, \n",
    "                                      replace=False, # sample without replacement to prevent repeated data\n",
    "                                      n_samples=number_in_minority_class,    # to match minority class\n",
    "                                      random_state=123) # reproducible results\n",
    "\n",
    "# Join the resampled and original majority and minority classes\n",
    "balanced_data = pd.concat([majority_class_downsampled, minority_class_df], axis=0, sort=True)\n",
    "\n",
    "balanced_data = balanced_data.reset_index(drop=True)\n",
    "\n",
    "# Display new class counts\n",
    "balanced_data[\"target_class\"].value_counts()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "You have been given a data set which contains information about bank customers. Your task is to build a model using the K-nearest neighbour classifier that will predict, based on the data given, whether or not the customer stopped using the service. \n",
    "\n",
    "Use the data set `\"../../data/churn.csv\"`.\n",
    "\n",
    "The data set has fifteen initial features with the **`Attrition_Flag`** attribute being the target variable. The data contains missing values, unique values, and a range of data types which will require preparing. \n",
    "\n",
    "Your goal is to use the material in this course to achieve a weighted average F1 classification score of greater than XXX on a 80:20 train-test split.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<ul>\n",
    "  <li>Attempt to complete the whole problem before looking at the model answer. \n",
    "        </li>\n",
    "  <li>Can you improve on the model answer's score? \n",
    "</li>\n",
    "  <li>Does logistic regression perform better or worse on this problem?\n",
    "</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import your libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import your data and explore its properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# handle missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# engineer your features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale your features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select your features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# improve your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for this case study was obtained from [kaggle](https://www.kaggle.com/sakshigoyal7/credit-card-customers) and editted to make it easer for learning.\n",
    "\n",
    "### Summary\n",
    "\n",
    "This case study should have given you experience with completing a machine learning problem on your own using the different topics we have discussed in this course. It is important to be able to improve your model by changing and adapting any of the steps in the workflow. The way to improve your model is not solely a hyper parameter optimisation problem, there can be barriers and reducible error throughout the different processes involved.\n",
    "\n",
    "We introduced a new type of model, the K-nearest neighbour algorithm, which is a powerful tool for classification problems. This course has not covered a wide range of models, but the key requirements for learning new models should be clear. Before just plugging in a new model into your workflow it is important to know:\n",
    "* How the model works at a high level\n",
    "* What inputs/data processing is required\n",
    "* What are the key hyperparameters that will effect it's performance\n",
    "\n",
    "Some other useful model families that are worth exploring and testing include:\n",
    "* [Decision trees](https://scikit-learn.org/stable/modules/tree.html) (`tree.DecisionTreeClassifier`, `tree.DecisionTreeRegressor`)\n",
    "* [Na&#239;ve Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) (`naive_bayes.CategoricalNB`)\n",
    "* [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html) (`svm.LinearSVC`, `svm.LinearSVR`)\n",
    "\n",
    "Further study in Machine Learning to a more intermediate level would include topics such as:\n",
    "* [Multiclass problems](https://scikit-learn.org/stable/modules/multiclass.html) (more than 2 classes)\n",
    "* [Multilabel problems](https://scikit-learn.org/stable/modules/multiclass.html) (more than 1 target variable)\n",
    "* [Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html) (combining many models to improve performance)\n",
    "* [Pipelines](https://scikit-learn.org/stable/modules/compose.html) (efficient flow of ML steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b><font size=\"4\"> Next: Case Study B</font> </b> \n",
    "<p> \n",
    "The next case study will allow you to experience using feature engineering techniques to improve model performance.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
