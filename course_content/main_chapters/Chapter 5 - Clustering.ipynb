{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<a href=\"#Measuring-Distance\"><font size=\"+0.5\">Measuring Distance</font></a>\n",
    "* Metrics\n",
    "* Distance Functions\n",
    "\n",
    "<a href=\"#KMeans-Clustering\"><font size=\"+0.5\">KMeans Clustering</font></a>\n",
    "* Algorithm\n",
    "* Implementing KMeans\n",
    "\n",
    "<a href=\"#Measuring-Performance\"><font size=\"+0.5\">Measuring Performance</font></a>\n",
    "* Known Class Membership - Extrinsic\n",
    "* Unknown Class Membership - Intrinsic\n",
    "\n",
    "<a href=\"#Density-Based-Clustering\"><font size=\"+0.5\">Density Based Clustering</font></a>\n",
    "* DBSCAN Properties\n",
    "* Implementing DBSCAN\n",
    "* DBSCAN Challenges\n",
    "\n",
    "<a href=\"#Hierarchical-Clustering\"><font size=\"+0.5\">Hierarchical Clustering</font></a>\n",
    "* Algorithm\n",
    "* Linkage\n",
    "* Dendrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><font size=6>Unsupervised Learning</font></h1></center>\n",
    "<center><h1><font size=7>Clustering</h1></center>\n",
    "\n",
    "Clustering is an Unsupervised Learning technique; here, the aim is to group our data points into classes/clusters based on their similarity/distance. We want to maximize the similarity of data grouped together and the separatation/distance from those data points that are not similar.\n",
    "\n",
    "These groups that we assign to the data points are called clusters, and are based on measuring how alike data points are in our data space. How we measure this alikeness is down to the method of clustering used, most of which use the *distance between data points* in some manner. \n",
    "\n",
    "Clustering allows us to find structures and similarities in data by using simple rules to group data together. Doing so can tell us about hidden properties in the data and groups that may not intuitively be obvious. \n",
    "\n",
    "Whereas in supervised learning, we used attributes of data to predict classes (based on trained classification model), in clustering we will use attributes of data to *create* classes. Our algorithms may not be able to produce names for our new classes, but their existence can lead to a greater understanding of the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "The aims of this chapter are for participants to:\n",
    "\n",
    "* Understand how distances can be measured.\n",
    "* Be comfortable applying K-Means clustering.\n",
    "* Understand how clustering performance can be measured, intrinsically and extrinsically.\n",
    "* Be comfortable applying Density based Clustering.\n",
    "* Be comfortable applying Hierarchical  Clustering.\n",
    "* Be able to create dendrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Note\n",
    "\n",
    "In this chapter there is a significant amount of plotting involved in order to demonstrate visually clusters of data. The to generate these plots is similar and repetative. For the benefit of learning how to plot in this style and for those doing self-learning this code has been kept within the course rather than moved to functions elsewhere. This is not best practice in programming style for a project, but useful in our learning context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Distance\n",
    "\n",
    "Measuring the distance between two data points was briefly discussed in Case Study A from Introduction to Machine Learning, used in the K-Nearest Neighbour Algorithm.\n",
    "\n",
    "Quantifying the distance between data points is fundamental to clustering techniques, \n",
    "therefore, we will introduce how to do this more formally in this chapter.\n",
    "\n",
    "In the \"real world\" we have a clear understanding of distance, it comes intuatievly. However, how we measure distance physically is only one of several methods that exist with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "We are going to define a metric as a function that measures the distance between two points, whilst having several mathematical properties.\n",
    "\n",
    "The properties we are going to discuss are necessary so that the ways we quantify the differences between points are valid.\n",
    "\n",
    "Without the properties we could come up with any mathematical rule and count it as a distance function without it having any real meaning.\n",
    "\n",
    "We are going to denote the distance function as $\\mathbf{d}$ and the two points in question are $a$ and $b$. \n",
    "\n",
    "Therefore the distance between $a$ and $b$ is:\n",
    "\n",
    "$$\\mathbf{d}(a,b)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The properties our distance function have are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetry\n",
    "\n",
    "The measurements of the distance must be symmetric, so which point is in each position does not change the distance. The distance from $a$ to $b$ must be the same as the distance from $b$ to $a$.\n",
    "\n",
    "$$\\mathbf{d}(a,b) = \\mathbf{d}(b,a)$$\n",
    "\n",
    "<img src=\"../../images/symmetric.png\"  width=\"500\" height=\"500\" alt=\"Visual representation of symetric quality of distances.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negativity\n",
    "\n",
    "Intuitively, we cannot have negative distance measurements, so our distance will always be greater than zero.\n",
    "\n",
    "$$\\mathbf{d}(a,b) \\geq 0$$\n",
    "\n",
    "<img src=\"../../images/nonnegative.png\"  width=\"250\" height=\"200\" alt=\"Visual representation non-negative property of distance metric.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalence\n",
    "\n",
    "If two points are at the same points in space then they are equivalent. This means if we have duplicate data the distance between the points will be zero, and any time we have a distance of zero, we have equivalent data.\n",
    "\n",
    "$$\\mathbf{d}(a,b) = 0 \\implies a = b$$\n",
    "\n",
    "<img src=\"../../images/equivalence.png\"  width=\"450\" height=\"500\" alt=\"Visual representation of the equivalence of two data points in the same place being equivalent.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangular inequality\n",
    "\n",
    "This final property states that the direct distance between two points is shorter than or equal to any distance via an intermediary point. \n",
    "\n",
    "$$\\mathbf{d}(a,b) \\leq \\mathbf{d}(a,c) + \\mathbf{b}(c,b)$$\n",
    "\n",
    "<img src=\"../../images/triangular.png\"  width=\"250\" height=\"250\" alt=\"Visual representation of triangular property of distance metric\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance functions\n",
    "\n",
    "Now that we know what makes a valid distance function we are going to look at some commonly used measures. \n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "The first is the **euclidean distance**, which is the distance in a straight line measurement in \"euclidean space\".\n",
    "\n",
    "We can calculate the euclidean distance by finding the square root of the sum of squared differences in each of the $n$ dimensions/coordinates of the data.\n",
    "\n",
    "The euclidean distance is often referred to as the $L_2$ distance or \"norm\", we will see why later.\n",
    "\n",
    "$$L_2 = \\mathbf{d}(\\mathbf{a},\\mathbf{b}) = \\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + ... + (a_n - b_n)^2}$$\n",
    "\n",
    "$$L_2 = \\mathbf{d}(\\mathbf{a},\\mathbf{b}) = \\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^2} $$\n",
    "\n",
    "\n",
    "<img src=\"../../images/L2.png\"  width=\"250\" height=\"250\" alt=\"Visual representation L2 measurement in Euclidean space\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan Distance\n",
    "\n",
    "This second method for measuring distance takes it's name from the street grid layout of Manhattan in New York, as well as many other cities. The intuition for this method is that distance travelled cannot always be described by the shortest path in space, but rather has to follow grid lines. \n",
    "\n",
    "The Manhattan distance is also called the Taxicab metric or snake distance and we will use the notation $L_1$.\n",
    "\n",
    "There can often be more than one path between two points which have the same Manhattan Distance. \n",
    "\n",
    "The distance is calculated as **the sum of lengths between two points in each dimension**. This uses the *absolute value* of the lengths, rather than the squared sum, it is given by:\n",
    "\n",
    "$$ L_1 = \\mathbf{d}(\\mathbf{a},\\mathbf{b}) = |a_1 - b_1| + |a_2 - b_2| + ... + |a_n - b_n| $$\n",
    "\n",
    "$$ L_1 = \\mathbf{d}(\\mathbf{a},\\mathbf{b}) = \\sum_{i=1}^{n} |a_i - b_i| $$\n",
    "\n",
    "<img src=\"../../images/L1.png\"  width=\"250\" height=\"250\" alt=\"Visual representation L1 measurement in Euclidean space\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalise\n",
    "\n",
    "We can generalise the $L_1$ and $L_2$ norms into one equation used for measuring distances. This is known as the *Minkowski* distance, where we have the parameter $p$, which specifies the type of distance used.\n",
    "\n",
    "For the Manhattan distance $ p = 1$.\n",
    "\n",
    "For the Euclidean distance $ p = 2$.\n",
    "\n",
    "$$ \\mathbf{d}(\\mathbf{a},\\mathbf{b}) = (\\sum_{i = 1}^{n} |a_i - b_i|^p)^{\\frac{1}{p}}$$\n",
    "\n",
    "We will be looking at these two methods for measuring distance throughout the course as they can be implemented in different clustering methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans Clustering\n",
    "\n",
    "The first example of clustering algorithm we will explore is the KMeans method. The *K* refers to how many clusters we want the algorithm to find for us. \n",
    "\n",
    "We specify how many clusters we want to find, the algorithm will take the data and work out which points of the data belong to which cluster. \n",
    "\n",
    "At the end of our algorithm we will have each of our data points having membership of a class. Our algorithm will not tell us what the different classes mean, but we can interpret the meaning using domain knowledge.\n",
    "\n",
    "This method uses the mean position of each cluster to assign membership of the cluster, we will call this mean position a ***centroid***.\n",
    "\n",
    "<img src=\"../../images/kmeans_easy.png\"  width=\"300\" height=\"400\" alt=\"Simple diagram of three well separated classes.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "**Input**\n",
    "* $k$ - number of clusters desired\n",
    "* $X$ - training data of $n$ rows and $p$ columns\n",
    "\n",
    "**Step 1 - Initialise Centroids**\n",
    "\n",
    "Select $k$ initial samples of the data as the starting centroids. This can be done either *randomly* or using a more complex algorithm approach.\n",
    "\n",
    "<img src=\"../../images/kmeans1.png\"  width=\"200\" height=\"200\" alt=\"First step of kmeans algorithm, initialising centroids.\">\n",
    "\n",
    "**Step 2 -  Assign To Nearest Centroid**\n",
    "\n",
    "Calculate the *euclidean* distance between all points and each centroid. Assign each point to it's nearest centroid.\n",
    "\n",
    "<img src=\"../../images/kmeans2.png\"  width=\"200\" height=\"200\" alt=\"Second step of kmeans algorithm, assigning nearest centroid\">\n",
    "\n",
    "**Step 3 - Calculate New Centroids**\n",
    "\n",
    "Now that we have each data point as a member of a class, we can calculate the new centroid of that class as the mean position of all data points with that class.\n",
    "\n",
    "<img src=\"../../images/kmeans3.png\"  width=\"200\" height=\"200\" alt=\"Third step of kmeans algorithm, Calculating new centroids\">\n",
    "\n",
    "**Step 4 - Repeat Steps 2 & 3**\n",
    "\n",
    "Now that we have the new centroid values we can reassign each data point to an class based on it's distance to each newly calculated centroid. Once the assignment takes place new centroids are calculated and so on.\n",
    "\n",
    "This process continues for some specified number of iterations or until some convergence condition is met.\n",
    "\n",
    "<img src=\"../../images/kmeans4.png\"  width=\"400\" height=\"200\" alt=\"Fourth step of kmeans algorithm, repeating assignment and calculation\">\n",
    "\n",
    "The output of this algorithm (which data points belong to which final class) will be dependent on the initial selection (seed) of centroids chosen. For this reason the algorithm should be run multiple times and the output checked to avoid poorly generalised models.\n",
    "\n",
    "\n",
    "How many steps this algorithm will take to run is dependent on several factors. These are:\n",
    "\n",
    "* $n$ - The number of data points\n",
    "* $K$ - The number of clusters\n",
    "* $d$ - How many dimensions (axis) each data point has\n",
    "* $i$ - The number of iterations until convergence (how many repeats, determined by how well clustered the data is)\n",
    "\n",
    "How long it will take for our algorithm to run is linear with $n$, $K$, $d$ and $i$. Therefore as all these increase so will our algorithm's runtime.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using K-Means\n",
    "\n",
    "As with most machine learning algorithms we do not need to implement this method ourselves, `sklearn` provides a class with many parameters we can use.\n",
    "\n",
    "We are going to look at another toy data set; `clusters.csv`, to explore the algorithm.\n",
    "\n",
    "*Please ensure you load the packages below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "generic_data = np.loadtxt(\"../../data/clusters.csv\", delimiter=\",\")\n",
    "\n",
    "# We have two data columns and a true class column\n",
    "generic_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to separate the data and the true label values for now. Also we can standard scale the data to make for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the first two columns of the data and scale it\n",
    "X = StandardScaler().fit_transform(generic_data[:,0:-1])\n",
    "labels_true = generic_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data distribution over the two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Unknown property density",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-746d7c54f453>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Plot distribution of each feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"$x_1$\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rebeccapurple\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"$x_2$\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dodgerblue\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python36\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1895\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1896\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1898\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python36\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mhist\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   6387\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6388\u001b[0m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6389\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6390\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlbl\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6391\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlbl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python36\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, props)\u001b[0m\n\u001b[0;32m    883\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m             ret = [_update_property(self, k, v)\n\u001b[1;32m--> 885\u001b[1;33m                    for k, v in props.items()]\n\u001b[0m\u001b[0;32m    886\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meventson\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python36\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    883\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m             ret = [_update_property(self, k, v)\n\u001b[1;32m--> 885\u001b[1;33m                    for k, v in props.items()]\n\u001b[0m\u001b[0;32m    886\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meventson\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python36\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36m_update_property\u001b[1;34m(self, k, v)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'set_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown property %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    879\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Unknown property density"
     ]
    }
   ],
   "source": [
    "# Create a density plot for both dimensions\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4), sharey=True)\n",
    "\n",
    "# Plot distribution of each feature\n",
    "axes[0].hist(x=X[:,0], bins=20, density=True, label=\"$x_1$\", color=\"rebeccapurple\")\n",
    "axes[1].hist(x=X[:,1], bins=20, density=True, label=\"$x_2$\", color=\"dodgerblue\")\n",
    "\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "axes[0].set_xlabel(\"$x_1$\")\n",
    "axes[1].set_xlabel(\"$x_2$\")\n",
    "fig.suptitle(\"Distribution of each dimension\")\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each dimension there are two clear groupings of the data. One grouping in each dimension has a higher density than the other. Plotting the data this way hides some of the structure of the data, so as a result we can plot the dimensions against each other to understand the relationship between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the data in 2 dimensions\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x=X[:,0], y=X[:,1], c=\"navy\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data it appears that there are 3 groupings of data, so we will select $k = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 3\n",
    "\n",
    "# Fit clusterer using basic parameters\n",
    "km_clusterer = KMeans(n_clusters=k, \n",
    "                      init=\"random\", # method for selecting initial cluster points\n",
    "                      max_iter=1, # iterations before stopping\n",
    "                      random_state=123).fit(X)\n",
    "\n",
    "labels_found_3 = km_clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We find as many labels as we give rows of X\n",
    "labels_found_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our labels are the group found for each data point\n",
    "labels_found_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the clusters that have been found with only one iteration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Give each label value a different colour to show which value\n",
    "# was found in clustering. \n",
    "\n",
    "label_colour_pairs = [(0,\"green\"), (1,\"blue\"), (2,\"orange\")]\n",
    "\n",
    "for label_value, label_colour in label_colour_pairs:\n",
    "    \n",
    "    # Plot all of the data points with a specific label value\n",
    "    X_clust = X[labels_found_3 == label_value]\n",
    "    \n",
    "    # Colour based on label value\n",
    "    ax.scatter(x=X_clust[:,0], y=X_clust[:,1], \n",
    "               c=label_colour, label=str(label_value))\n",
    "    \n",
    "ax.set_title(\"Found Cluster Labels $K=3$\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual classes are given below. It's important to note that the clustering *did not predict which point is in which target class, but rather grouped them into their own classes*. \n",
    "\n",
    "Each label for a cluster doesn't have an inherent meaning. The below graph shows the target's true classes. Note that there is some difference between the found clusters in the data, and the true label values. \n",
    "\n",
    "In clustering you do not have a true value / target as the purpose is to understand the natural grouping of data based on distances between points. We are using the true target values below for learning purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Note - the model and true values order of classes may differ\n",
    "# True class: 0 != cluster: 0.\n",
    "# Each time we run the clustering we may get different orders\n",
    "\n",
    "label_colour_pairs = [(0,'royalblue'), (1,'goldenrod'), (2,'seagreen')]\n",
    "\n",
    "for label_value, label_colour in label_colour_pairs:\n",
    "    \n",
    "    # Plot each data grouped by label\n",
    "    X_clust = X[labels_true == label_value]\n",
    "    ax.scatter(x=X_clust[:,0], y=X_clust[:,1], \n",
    "               c=label_colour, label=str(label_value))\n",
    "    \n",
    "ax.set_title(\"True Class Values\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the orderings (0, 1, 2) will differ between found clusters and true classes.\n",
    "\n",
    "Compare the found clusters and true class values, where is the clustering getting the groupings wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 1:</font></b> <p> \n",
    "Using the data imported below, visualise X. \n",
    "</p><p>\n",
    "Without looking at the true label values perform K-means clustering on the data with $K=2$. \n",
    "</p><p>\n",
    "Visualise the results of the clustering labels assigned as shown in previous examples. Change the value of $K$ to a more appropriate value and compare the results.\n",
    "\n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import and separate data\n",
    "unknown_clusters_data = np.loadtxt(\"../../data/exercise_one_clusters.csv\", delimiter=\",\")\n",
    "X, _true_labels= unknown_clusters_data[:,0:-1], unknown_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Visualise X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform the clustering, produce group labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualise your clustering results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find a better value of K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Performance\n",
    "\n",
    "As with supervised learning it is important to have a way to properly quantify how well a model / algorithm has performed. This is so we can compare different models. \n",
    "\n",
    "There are broadly two different ways to measure the performance of a clustering algorithm.\n",
    "\n",
    "The first is a set of methods that compare the output cluster memberships with a value of true class memberships. You can think of this in a similar way to how we compare predictions and true values in supervised learning, except we cannot use the *exact true value directly*, but whether the clusters separate similarly to the true classes. These methods are called extrinsic, or external.\n",
    "\n",
    "The second is to look at properties of the clusters themselves, evaluating the similarity of data placed within the same cluster. These methods are called intrinsic or internal.\n",
    "\n",
    "The extrinsic methods are *only possible where ground truth data exists*. For this reason we cannot always use these types of methods, but can always use intrinsic methods. \n",
    "\n",
    "It is important to note that if we have the true class values of a data set and wanted to predict new instances, we would typically use supervised learning rather than clustering. However, in some cases we may have a subset of true values and are trying to compare models on a baseline.\n",
    "\n",
    "Labelling data is expensive, if we always had labelled data we wouldn't need to predict anything! So often we will use some labelled data to help evaluate our clustering performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Known Class Membership - Extrinsic\n",
    "\n",
    "It is important to note that if we have the true class values of a data set and wanted to predict new instances, we would typically use supervised learning rather than clustering. However, in some cases we may have a subset of true values and are trying to compare models on a baseline.\n",
    "\n",
    "There are a number of extrinsic evaluation methods, we will be exploring the Adjusted Rand Index.\n",
    "\n",
    "### Adjusted Rand Index Properties\n",
    "The Adjusted Rand Index (ARI) takes as an input the found cluster membership and true class values.\n",
    "\n",
    "- The score is bounded between [-1, 1]. A good ARI, indicating similarity between clusters and classes, is near 1. A bad ARI, indicating no similarity between clusters and classes is negative.\n",
    "\n",
    "- A random selection of clustering compared to true classes will give an ARI of near 0. This is important as it allows us to compare out clustering to a baseline selection.\n",
    "\n",
    "- We do not need to know anything about the structure of the clusters or classes to perform this measure.\n",
    "\n",
    "*The important information about the ARI is discussed above, for more information on how it is calculated see below.* \n",
    "\n",
    "### ARI formula\n",
    "\n",
    "We first start with the Rand Index $RI$, then adjust it.\n",
    "\n",
    "We have $n$ data samples. \n",
    "\n",
    "Each sample has an associated cluster, in the set of clusters $K$, and a true class within the set $T$. \n",
    "\n",
    "Consider a set of all pairs of samples, such that all pairs represent all possible combinations of data points. The size of these combinations is given by $C_{2}^{n}$ ($n$ combination 2, where order does not matter).\n",
    "\n",
    "The number of combinations of data points where the pairs are **within the same** cluster (in $K$) **and the same** true class (in $T$) is $\\alpha$. \n",
    "\n",
    "The number of combinations of data points where the pairs are **not in the same** cluster (in $K$) **nor in the same** true class (in $T$) is $\\beta$. \n",
    "\n",
    "The $RI$ is therefore given by:\n",
    "\n",
    "$$ RI = \\frac{\\alpha + \\beta}{C_{2}^{n}} $$\n",
    "\n",
    "Which will have be bounded by $[0, 1]$.\n",
    "\n",
    "This however will give us misleading scores if we have randomly selected cluster membership the $RI$ will not necessarily be near zero. \n",
    "\n",
    "If we produce a random set of clusterings there will be some values that are correct for $\\alpha$ (same cluster, same true cluster) and for $\\beta$ (wrong cluster, wrong true cluster). This means given random chance we will have $\\alpha + \\beta > 0$ then $RI > 0$,\n",
    "\n",
    "Therefore we take into account the expected random $RI$, ($E[RI]$), and the maximum possible $RI$ to normalize our value.\n",
    "\n",
    "The Expected Random RI represents the average RI given all permutations of clustering labels. This helps give a more interpretable score, with random asignment being near zero.\n",
    "\n",
    "$$ ARI = \\frac{RI - E[RI]}{max(RI) - E[RI]} $$\n",
    "\n",
    "Which will now be bounded by $[-1, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARI Implementation\n",
    "\n",
    "Just like all other things we have discussed in this course there is of course an function provided by `sklearn` to find the ARI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Scale our X features for ease of clustering\n",
    "X = StandardScaler().fit_transform(generic_data[:,0:-1])\n",
    "\n",
    "# Access the true labels from the raw data\n",
    "labels_true = generic_data[:,-1]\n",
    "\n",
    "# Compare extrinsically the true and found groupings\n",
    "k_3_score = adjusted_rand_score(labels_true, labels_found_3)\n",
    "\n",
    "print(\"K=3 Adjusted Rand Index Score: \", round(k_3_score, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with a ARI of >0.95 the clustering with $k=3$ performs extremely well (how convenient!).\n",
    "\n",
    "This can be compared to a random selection of cluster membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import choices\n",
    "\n",
    "# Generate random labels from possible labels\n",
    "labels_random_list = choices(np.unique(labels_true).astype(int), k=len(labels_true))\n",
    "\n",
    "# Convert to numpy array for consistency and plotting\n",
    "labels_random = np.array(labels_random_list)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "label_colour_pairs = [(0,'green'), (1,'blue'), (2,'orange')]\n",
    "\n",
    "\n",
    "for label_value, label_colour in label_colour_pairs:\n",
    "    \n",
    "    X_clust = X[labels_random == label_value]\n",
    "    \n",
    "    ax.scatter(x=X_clust[:,0], y=X_clust[:,1], \n",
    "               c=label_colour, label=str(label_value))\n",
    "    \n",
    "ax.set_title(\"Random Label Values\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_score = adjusted_rand_score(labels_true, labels_random)\n",
    "print(\"Random Label Adjusted Rand Index:\", round(random_score, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows that our ARI will be near $0$ when a random sample of cluster membership is chosen.\n",
    "\n",
    "We can see below what the difference would be with a different value of $K$.\n",
    "\n",
    "This produces a result better than random, but worse than $K=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform another clustering on the data with a different K\n",
    "k = 2\n",
    "\n",
    "# Selecting basic parameters \"random\" and max_iter=1 to demonstrate simplified algorithm\n",
    "km_clusterer = KMeans(n_clusters=k, \n",
    "                      init=\"random\", \n",
    "                      max_iter=1, \n",
    "                      random_state=123).fit(X)\n",
    "\n",
    "# Generate labels from clustering\n",
    "labels_found_2 = km_clusterer.labels_\n",
    "\n",
    "label_colour_pairs = [(0,'lightcoral'), (1,'darkslateblue')]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Iterate over the different label values and corresponding colours\n",
    "for label_value, label_colour in label_colour_pairs:\n",
    "    \n",
    "    X_clust = X[labels_found_2 == label_value]\n",
    "    \n",
    "    ax.scatter(x=X_clust[:,0], y=X_clust[:,1], \n",
    "               c=label_colour, label=str(label_value))\n",
    "    \n",
    "ax.set_title(\"Found Cluster Labels $K=2$\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_2_score = adjusted_rand_score(labels_true, labels_found_2)\n",
    "\n",
    "print(\"K=2 Adjusted Rand Index Score:\", round(k_2_score, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other extrinsic methods (use true values) for measuring cluster performance include:\n",
    "\n",
    "* Mutual Information (normalise, adjusted)\n",
    "* Homogeneity, Completeness, V-measure (their harmonic mean)\n",
    "* Fowlkes-Mallows Index\n",
    "\n",
    "These have been excluded for brevity, but are worth exploring in your own time. Each has benefits and drawbacks just like any other evaluation metric discussed. A basic description and implementation can be found [in the sklearn documentation](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 2:</font></b> <p> \n",
    "Using the data imported below and the ARI work out what the best value for $K$ is. Plot the results of the best $K$. \n",
    "</p><p>\n",
    "Using the true labels work out how many actual groups in the data there are, is this the same as what you found? Plot the true labels and compare results.\n",
    "</p><p>\n",
    "**HINT** The np.unique() function gives you the unique elements in an array. Remember, the exact numerical label of each cluster isn't important, but whether points belong to the same group.\n",
    "  \n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import and separate data\n",
    "unknown_clusters_data = np.loadtxt(\"../../data/exercise_two_clusters.csv\", delimiter=\",\")\n",
    "X, true_labels= unknown_clusters_data[:,0:-1], unknown_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "# Find the best value for K using ARI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the found labels for your value of K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate how many true labels there are\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the true labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unknown Class Membership - Instrinsic\n",
    "\n",
    "For intrinsic evaluation we no longer have the true labels. This means we need to generate meaningful quantities from the data and clusters generated. We can do this using the distances between different positions within the data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score Properties\n",
    "\n",
    "The silhoette coefficient is a value that can be calculated for each data point in the set. When the average of all coefficients is found this gives an overall indication of how well clustering has performed.\n",
    "\n",
    "The produced silhouette score is bound between $[-1, 1]$, with $+1$ corresponding to the \"best\" and $-1$ the \"worst\". A value around $0$ indicates a high amount of crossing overlapping clusters (we may not see this frequently in KMeans).\n",
    "\n",
    "Values that are produced are therefore high when our clusters are well separated, and low when not.\n",
    "\n",
    "We can only calculate the silhouette score on data where $$2 \\leq k \\leq n - 1$$ with $k$ as the number of labels and $n$ the number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score Formula\n",
    "\n",
    "We first look at calculating a single silhouette coefficient for one data point.\n",
    "\n",
    "$a$ is the *mean **intra** cluster distance*. This is the sum of all the distances between all points within the cluster the data point is in divided by the number of points in that cluster.\n",
    "\n",
    "$b$ is the *mean **nearest** cluster distance*. This is sum of all the distances between that data point and the points within the **nearest** cluster that the point is **not a member of**. This is then divided by the number of data points in that nearest cluster.\n",
    "\n",
    "The coefficient is therefore given by:\n",
    "\n",
    "$$ SC = \\frac{b - a}{max(a, b)}$$\n",
    "\n",
    "We then sum up the coefficients of all data points to produce the mean value, the overall silhouette score.\n",
    "\n",
    "$$SS = \\frac{\\sum^{n}_{1}\\frac{b - a}{max(a, b)}}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score Implementation\n",
    "\n",
    "We will now look at how to use the Silhouette Score using the $K=3$ model fitted earlier. We can specify which metric we want to use when calculating the distances depending on what we think might be most appropriate.\n",
    "\n",
    "The metrics we can use are given by scikit-learn [here](https://scikit-learn.org/0.15/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html).\n",
    "\n",
    "Below we are going to calculate the baseline silhouette scores. These are created using the true labels, if the true labels give low silhouette scores then we shouldn't expect the clustered scores to be high. This means that the data may be inherently difficult to cluster.\n",
    "\n",
    "If our clustering can give a comparable silhouette scores to the true label scores then they are likely good groupings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Prepare raw data for clustering\n",
    "X = StandardScaler().fit_transform(generic_data[:,0:-1])\n",
    "\n",
    "# Produce baseline true value silhouette scores\n",
    "sil_score_eucl = silhouette_score(X, labels_true, metric=\"euclidean\")\n",
    "sil_score_manhat = silhouette_score(X, labels_true, metric=\"manhattan\")\n",
    "\n",
    "print(\"Euclidean Silhouette Score: \", round(sil_score_eucl, 4))\n",
    "print(\"Manhattan Silhouette Score: \", round(sil_score_manhat, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how each different metric gives us a different score.\n",
    "\n",
    "We can see how the Silhouette Score changes with $K$ by training and evaluating a number of models with different $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will iterate over different values of K and produce a model for each value of K\n",
    "# Choosing a euclidean distance metric for evaluation\n",
    "\n",
    "k_values = list(range(2, 40))\n",
    "\n",
    "sil_values = []\n",
    "\n",
    "for K in k_values:\n",
    "    # Perform the clustering for a given value of K\n",
    "    km_clusterer = KMeans(n_clusters=K, init=\"random\", \n",
    "                          max_iter=5, random_state=123).fit(X)\n",
    "    \n",
    "    labels_pred = km_clusterer.labels_\n",
    "    # Calculate the silhouette score for associated k clusters\n",
    "    eucl_sil_score = silhouette_score(X, labels_pred, metric=\"euclidean\")\n",
    "    # Add the score the list of scores\n",
    "    sil_values.append(eucl_sil_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plt.plot(k_values, sil_values, c=\"darkgreen\")\n",
    "\n",
    "ax.set_title(\"Silhouette Score for different K\")\n",
    "ax.set_xlabel(\"K\")\n",
    "ax.set_ylabel(\"Euclidean Silhoette Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that there is a clear best value of $K$ in this data set, shown by the highest Euclidean Silhouette score at $K = 3$. \n",
    "\n",
    "Even though our data is quite clearly three clusters, the score in this case is only ~0.65 because each cluster is not largely separated from each other. This shows clusters that are close together will score more poorly that those far apart.\n",
    "\n",
    "This method of evaluating the performance of a range of $K$ values is a useful method of finding an optimal $K$. Using the silhouette score tells which number of clusters maximises the mean distance between cluster points. (This is in effect a manual grid search we can visualise!)\n",
    "\n",
    "Even though this can be powerful, it is important we look at the resulting clustering and determine for ourselves if a value of $K$ is good.\n",
    "\n",
    "\n",
    "Other intrinsic methods for cluster evaluation include:\n",
    "\n",
    "* Variance Ratio Criterion (Calinski-Harabasz Index)\n",
    "* Davies-Bouldin Index (in this measure low is good!)\n",
    "\n",
    "\n",
    "As with all evaluation, you should consider what it is you are trying to evaluate (what is important to your model / research / problem) then select a method that aligns with this and will help you maximise this quantity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 3:</font></b> <p> \n",
    "Using the data imported below, perform KMeans clustering with $K=5$, calculate the silouette score using both manhattan and euclidean distances.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "unknown_clusters_data = np.loadtxt(\"../../data/exercise_three_clusters.csv\", delimiter=\",\")\n",
    "X, _true_labels= unknown_clusters_data[:,0:-1], unknown_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "# Perform Kmeans clustering on the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate labels and silhouette score using both metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Based Clustering\n",
    "\n",
    "KMeans clustering is merely one of a range of methods that can be used to create groupings. It is a powerful method, but it does have some inherent problems and areas where it does not perform well. \n",
    "\n",
    "KMeans clustering excells where there is a *clear separation between classes when taking into account the distance from the center*. You can think of this as clustering \"as the crow flies\". \n",
    "\n",
    "This is not a good property for some data, for example the two dimensional data set below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "moon_data = np.loadtxt(\"../../data/halfmoons.csv\", delimiter=\",\")\n",
    "\n",
    "# Get the first two columns of the data and scale it\n",
    "X_moon = StandardScaler().fit_transform(moon_data[:,0:2])\n",
    "\n",
    "moon_true_labels = moon_data[:,-1]\n",
    "\n",
    "label_colour_pairs = [(0,'seagreen'), (1,'purple')]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "# Specify a certain colour for each class\n",
    "for label_value, label_colour in label_colour_pairs:\n",
    "    \n",
    "    X_clust = X_moon[moon_true_labels == label_value]\n",
    "    \n",
    "    ax.scatter(x=X_clust[:,0], y=X_clust[:,1], \n",
    "               c=label_colour, label=str(label_value))\n",
    "\n",
    "ax.set_title(\"True Class Values\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this data is not as simply grouped as previous examples, but why? This is because the center of each cluster is actually very near data of a different class. K-means does not perform well even with the correct true $K$ value, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "\n",
    "# Train naive model\n",
    "km_clusterer_moon = KMeans(n_clusters=k, \n",
    "                           init=\"random\", \n",
    "                           max_iter=1, \n",
    "                           random_state=123).fit(X_moon)\n",
    "\n",
    "km_moon_labels = km_clusterer_moon.labels_\n",
    "\n",
    "label_colour_pairs = [(0,'magenta'), (1,'lightgreen')]\n",
    "\n",
    "# Plot found clusters\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "for label_value, label_colour in label_colour_pairs:\n",
    "    \n",
    "    X_clust = X_moon[km_moon_labels == label_value]\n",
    "    ax.scatter(x=X_clust[:,0], y=X_clust[:,1], \n",
    "               c=label_colour, label=str(label_value))\n",
    "    \n",
    "ax.set_title(\"K-Means clusters with $K=2$\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KMeans method is unable to consider local structure, it assumes that all clusters are convex, and that the centroid calculated is a significant measurement.\n",
    "\n",
    "Instead of measuring distances and creating centroids, a density based method takes into account how many data points are within the surrounding area of a \"core sample\". A core sample is a sample that is within an area of high density.\n",
    "\n",
    "The algorithm calculates the distance between points in and out of the core sample, this calculation spreads throughout the whole data in a search tree. \n",
    "\n",
    "The result of this approach is that density based methods can cluster in interesting shapes - so long as there is a dense enough link from points. \n",
    "\n",
    "The specific density based clustering algorithm we will be looking at is Density-Based Spatial Clustering of Applications with Noise (DBSCAN), a commonly used method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Properties\n",
    "\n",
    "Instead of clusters being convex, DBSCAN assumes that clusters are high density (many points in small area) and are separated by areas where there are fewer data points in a given area.\n",
    "\n",
    "Significantly, when compared to K-Means, a number of clusters **does not** need to be specified, DBSCAN will find this for us. \n",
    "\n",
    "Other than which distance metric to use, there are two characteristic values used by the DBSCAN algorithm.\n",
    "\n",
    "* The first is the \"reach distance\", the radius around a point that a density will be calculated from. As with the rest of our methods using distances, we want to be sure we are centering the data. \n",
    "\n",
    "* The second is the number of points within our reach distance we want to count an area as high density (includes the point in question).\n",
    "\n",
    "These two quantities, the number of points and surrounding area, allow the density around each point to be calculated. If the density is above the required amount then the new point is added to the cluster.\n",
    "\n",
    "$$density = \\frac{count}{space}$$\n",
    "\n",
    "Higher number of points or lower distance will require the data to be more dense in order to be added to the cluster. \n",
    "\n",
    "Some points are within the required distance, but not dense enough to be \"core samples\", these points are non-core samples and still part of the cluster, but on the fringes. \n",
    "\n",
    "Data points a distance further than the reach distance from a core sample are considered outliers by DBSCAN and not given a cluster (the label -1 is given to outliers).\n",
    "\n",
    "DBSCAN is not inherently stochastic, but label membership can depend on the order of core samples chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing DBSCAN\n",
    "\n",
    "Within `sklearn` the following conventions are used:\n",
    "\n",
    "* The \"reach distance\" is called `eps` (epsilon, $\\epsilon$)\n",
    "* The number of points required is called `min_samples`\n",
    "\n",
    "As these two quantities are reciprocal with regards to density, we can look to improve just one, such as `eps`.\n",
    "\n",
    "Changing `min_samples` can help impact the effect of noisy data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Define a value for eps\n",
    "reach_dist = 0.2\n",
    "\n",
    "# Run the clustering algorithm on data\n",
    "dbscan_clusterer = DBSCAN(eps=reach_dist, metric=\"euclidean\").fit(X_moon)\n",
    "\n",
    "db_moon_labels = dbscan_clusterer.labels_\n",
    "\n",
    "label_colour_pairs = [(-1, \"black\"), (0,'orchid'), (1,'lightseagreen'), (2, \"slateblue\"), (3, \"yellow\")]\n",
    "\n",
    "# Plot the found clusters and unlabelled data\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "for label_value, label_colour in label_colour_pairs:\n",
    "    \n",
    "    X_clust = X_moon[db_moon_labels == label_value]\n",
    "    \n",
    "    ax.scatter(x=X_clust[:,0], y=X_clust[:,1], c=label_colour, label=str(label_value))\n",
    "    \n",
    "ax.set_title(\"DBSCAN with eps={}\".format(reach_dist))\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of `-1` corresponds to \"No cluster\". This means the point has not been allocated into an existing cluster at the point the clustering algorithm finishes. In this method - not all points are near enough others to be counted. This property can be used to detect outliers in data.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this that the DBSCAN is not actually performing as well as we would like, it is showing there are three clusters when intuitively we can see two. There are also some outliers (black) which should be grouped in a main cluster.\n",
    "\n",
    "By iterating over the `eps` hyperparameter using a scoring function we can see what a better approximate value for `eps` might be. See the further reading section for more information on this plot.\n",
    "\n",
    "We are going to use the silhouette score to measure our clusters performance, then compare the best `eps` with our original clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a sequence of possible eps values in a range\n",
    "possible_eps_values = np.linspace(start=0.01, stop=0.5, num=50)\n",
    "possible_eps_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will iterate over different values of eps and produce a model for each iteration\n",
    "\n",
    "eps_values = []\n",
    "dbscan_sil_values = []\n",
    "\n",
    "# Generate a range of eps to try\n",
    "for reach_dist in possible_eps_values:\n",
    "    \n",
    "    # Create labels for each reach_dist\n",
    "    dbscan_clusterer = DBSCAN(eps=reach_dist, metric=\"euclidean\").fit(X_moon)\n",
    "    labels_pred = dbscan_clusterer.labels_\n",
    "    \n",
    "    # We can only use the silhouette score when there are >1 labels\n",
    "    if len(np.unique(labels_pred)) > 1:\n",
    "        eucl_sil_score = silhouette_score(X_moon, labels_pred, metric=\"euclidean\")\n",
    "        # Add the eps and score used to a list for plotting\n",
    "        eps_values.append(reach_dist)\n",
    "        dbscan_sil_values.append(eucl_sil_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(eps_values, dbscan_sil_values, c=\"blueviolet\")\n",
    "ax.set_title(\"Silhouette Score for different $eps$\")\n",
    "ax.set_xlabel(\"$eps$\")\n",
    "ax.set_ylabel(\"Euclidean Silhoette Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows us that the lower values of `eps` perform poorly with this data, with more reasonable scores greater that 0.15. In addition, the scores plateu around 0.3.\n",
    "\n",
    "Below we visualise a range of different `eps` values to show how the clustering changes with this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reach_distances = [0.18, 0.2, 0.3, 0.4]\n",
    "\n",
    "label_colour_pairs = [(-1, \"black\"), (0,'orchid'), (1,'lightseagreen'), (2, \"slateblue\"), (3, \"yellow\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,4), nrows=1, ncols=4, sharey=True)\n",
    "\n",
    "for axes_index, reach_dist in enumerate(reach_distances):\n",
    "    \n",
    "    # Fit the clusterer on the data\n",
    "    dbscan_clusterer = DBSCAN(eps=reach_dist, metric=\"euclidean\").fit(X_moon)\n",
    "    \n",
    "    # Generate the labels\n",
    "    db_moon_labels = dbscan_clusterer.labels_\n",
    "    \n",
    "    # Plot each label for each clusterer\n",
    "    for label_value, label_colour in label_colour_pairs:\n",
    "        \n",
    "        X_clust = X_moon[db_moon_labels == label_value]\n",
    "        \n",
    "        ax[axes_index].scatter(x=X_clust[:,0], y=X_clust[:,1], \n",
    "                               c=label_colour, label=str(label_value))\n",
    "        \n",
    "        ax[axes_index].set_title(\"$eps={}$\".format(reach_dist))\n",
    "        ax[axes_index].set_xlabel(\"$x_1$\")\n",
    "        ax[axes_index].set_ylabel(\"$x_2$\")\n",
    "        \n",
    "    \n",
    "    # Count the number unlabelled points and total classes\n",
    "    number_unlabelled = np.count_nonzero(db_moon_labels == -1)\n",
    "    number_unique_labels = len(np.unique(db_moon_labels[db_moon_labels != -1]))\n",
    "    \n",
    "    ax[axes_index].text(s=\"Classes: {}\".format(number_unique_labels), \n",
    "                        x=-2., y=-1.9, fontsize=10)\n",
    "    ax[axes_index].text(s=\"Unlabelled points: {}\".format(number_unlabelled), \n",
    "                        x=-2., y=-2.15, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare our `eps=0.2` and `eps=0.3` cluster labels you can see that there are two classes produced by `eps=0.3` which more naturally fits the data. In addition, there are no unclassed data points (denoted in black). \n",
    "\n",
    "By finding a more appropriate `eps` value we can group the data into more coherent groups.\n",
    "\n",
    "As the reach distance increases our allocations may be more resistant to noise, but also more likely to label lower density areas, that are potentially a different cluster. \n",
    "\n",
    "At a certain size of reach distance we end up labelling all our data in the same class, which is not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen the DBSCAN can be a powerful method for finding irregularly shaped clusters. When using this density based method we are making a different assumption about our data/labels than for KMeans.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 4:</font></b> <p> \n",
    "Using the data loaded below, using *eps=0.15* calculate the silhouette score of the DBSCAN clustering using euclidean distance for both DBSCAN and silhouette values. Visualise the found clusters\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "swirl_data = np.loadtxt(\"../../data/exercise_four_clusters.csv\", delimiter=\",\")\n",
    "X, _true_labels= swirl_data[:,0:-1], swirl_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualise the found clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Challenges\n",
    "\n",
    "When using a density based approach we expect our clusters are expected to have a similar density as each other. This helps us to tune an appropriate `eps`/`min_samples`. If this is not true then it can often be a challenge to find appropriate parameter values for the algorithm, and produce poor clustering labels.\n",
    "\n",
    "In the example below we look at a similar set of data to the one used to explore K-means clustering, with three distinct circular clusters. However, this data is slightly different:\n",
    "\n",
    "* The data in each cluster is \"noisier\", each grouping has a larger standard deviation.\n",
    "* One of the clusters, given by the green colour, is less dense. It has 70% the number of data points with that label compared to the other labels.\n",
    "\n",
    "We are first going to look at the clustering produced by K-means then compare that with DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "density_data = np.loadtxt(\"../../data/clusters_density.csv\", delimiter=\",\")\n",
    "density_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process the data\n",
    "X_density = StandardScaler().fit_transform(density_data[:,0:-1])\n",
    "labels_true = density_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), nrows=1, ncols=2, sharey=True)\n",
    "\n",
    "label_colour_pairs_true = [(0,'royalblue'), (1,'goldenrod'), (2,'seagreen')]\n",
    "\n",
    "# Plot true values\n",
    "for label_value, label_colour in label_colour_pairs_true:\n",
    "    \n",
    "    X_clust = X_density[labels_true == label_value]\n",
    "    \n",
    "    ax[0].scatter(x=X_clust[:,0], y=X_clust[:,1], \n",
    "                  c=label_colour, label=str(label_value))\n",
    "    \n",
    "ax[0].set_title(\"True Class Values\")\n",
    "ax[0].set_xlabel(\"$x_1$\")\n",
    "ax[0].set_ylabel(\"$x_2$\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Generate cluster labels using K-means\n",
    "k = 3\n",
    "\n",
    "# Use naive implementation for consistency\n",
    "km_clusterer = KMeans(n_clusters=k, \n",
    "                      init=\"random\", \n",
    "                      max_iter=1, \n",
    "                      random_state=123).fit(X_density)\n",
    "\n",
    "labels_pred = km_clusterer.labels_\n",
    "\n",
    "# Calculate silhouette score of found and true clusters\n",
    "kmeans_density_score = silhouette_score(X_density, labels_pred)\n",
    "true_score = silhouette_score(X_density, labels_true)\n",
    "\n",
    "print(\"Silhouette Score for K=3:\", kmeans_density_score.round(3))\n",
    "print(\"Silhouette Score for True labels:\", true_score.round(3))\n",
    "\n",
    "label_colour_pairs_found = [(0,\"blue\"), (1,\"orange\"), (2,\"green\")]\n",
    "\n",
    "# Plot found clusters\n",
    "for label_value, label_colour in label_colour_pairs_found:\n",
    "    \n",
    "    X_clust = X_density[labels_pred == label_value]\n",
    "    \n",
    "    ax[1].scatter(x=X_clust[:,0], y=X_clust[:,1], \n",
    "                  c=label_colour, label=str(label_value))\n",
    "    \n",
    "ax[1].set_title(\"Found Cluster Labels $K=3$\")\n",
    "ax[1].set_xlabel(\"$x_1$\")\n",
    "ax[1].set_ylabel(\"$x_2$\")\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-means clustering looks like it is performing well in this case. Our found clusters are actually more compact than the true values!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform the same process for looking at the effect of `eps` on our clustering metric we can get silhouette scores for DBSCAN. This will help us pick a good value for `eps`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we iterate over a range of `eps`, not all values of `eps` will give us a clustering that has more than one label. Some cluster distances will clump all data points in the same group. If that is so we cannot produce a silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lists to store plotting values\n",
    "eps_values = []\n",
    "dbscan_sil_values = []\n",
    "\n",
    "# Generate a range of eps to try\n",
    "for reach_dist in np.linspace(start=0.1, stop=1, num=50):\n",
    "    \n",
    "    # Create labels for each reach_dist\n",
    "    dbscan_clusterer = DBSCAN(eps=reach_dist, metric=\"euclidean\").fit(X_density)\n",
    "    labels_pred = dbscan_clusterer.labels_\n",
    "\n",
    "    # We can only use the silhouette score when there are >1 labels\n",
    "    if len(np.unique(labels_pred)) > 1:\n",
    "        eucl_sil_score = silhouette_score(X_density, labels_pred, metric=\"euclidean\")\n",
    "        # Add the eps and score used to a list for plotting\n",
    "        eps_values.append(reach_dist)\n",
    "        dbscan_sil_values.append(eucl_sil_score)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "plt.plot(eps_values, dbscan_sil_values, c=\"blueviolet\")\n",
    "ax.set_title(\"DBSCAN Silhouette Scores for different $eps$\")\n",
    "ax.set_xlabel(\"$eps$\")\n",
    "ax.set_ylabel(\"Euclidean Silhoette Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our plot above we can see that we get better values as we increase `eps` up to some point, with the best values approximately $eps > 0.37$.  \n",
    "\n",
    "But what we have done here is find a parameter based on some metric, we haven't actually checked what our clusters look like, or if they are grouping the data in an intuitive way.\n",
    "\n",
    "We could see that there are three main clusters in the data originally, does DBSCAN reproduce this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reach_distances = [0.24, 0.26, 0.3, 0.6]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,4), nrows=1, ncols=4, sharey=True)\n",
    "\n",
    "label_colour_pairs = [(-1, \"black\"), (0,'orchid'), \n",
    "                      (1,'lightseagreen'), (2, \"slateblue\"), \n",
    "                      (3, \"yellow\"), (4, \"cornflowerblue\")]\n",
    "\n",
    "# Produce a plot for each value of eps / reach distance\n",
    "for axes_index, reach_dist in enumerate(reach_distances):\n",
    "    \n",
    "    # Fit the clusterer on the data\n",
    "    dbscan_clusterer = DBSCAN(eps=reach_dist, metric=\"euclidean\").fit(X_density)\n",
    "    \n",
    "    # Generate the labels\n",
    "    labels_pred = dbscan_clusterer.labels_\n",
    "    \n",
    "    # Plot each label value for each clusterer\n",
    "    for label_value, label_colour in label_colour_pairs:\n",
    "        \n",
    "        X_clust = X_density[labels_pred == label_value]\n",
    "        \n",
    "        ax[axes_index].scatter(x=X_clust[:,0], y=X_clust[:,1], \n",
    "                               c=label_colour, label=str(label_value))\n",
    "        \n",
    "        ax[axes_index].set_title(\"$eps={}$\".format(reach_dist))\n",
    "        ax[axes_index].set_xlabel(\"$x_1$\")\n",
    "        ax[axes_index].set_ylabel(\"$x_2$\")\n",
    "    \n",
    "    # Count the number unlabelled points and total classes\n",
    "    number_unlabelled = np.count_nonzero(labels_pred == -1)\n",
    "    number_unique_labels = len(np.unique(labels_pred[labels_pred != -1]))\n",
    "    \n",
    "    ax[axes_index].text(s=\"Classes: {}\".format(number_unique_labels), \n",
    "                        x=-3., y=1.9, fontsize=10)\n",
    "    ax[axes_index].text(s=\"Unlabelled points: {}\".format(number_unlabelled), \n",
    "                        x=-3., y=2.25, fontsize=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the different values of `eps` shown within `reach_distances`. What you should be able to see is that our clustering is very sensitive to the value of `eps`, and none of our options produce the 3 clusters we were expecting.\n",
    "\n",
    "As a result, we would conclude that with this data set DBSCAN is probably not an appropriate choice in method.\n",
    "\n",
    "We could also see this by comparing the silhouette scores produced by different methods. For K-means with $K=3$ we get $s \\approx 0.5$ compared to the maximum from DBSCAN of $s \\approx 0.4$.\n",
    "\n",
    "As a result, we should always consider multiple clustering methods for our data, as the structure of the data will lend itself to some approaches more than others. By measuring the different methods performances using the same metrics we can then compare methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 5:</font></b> <p> \n",
    "Using the data loaded below, visualise the two true classes in the data set, why might these clusters be challenging to model?\n",
    "</p><p>\n",
    "Select a distance metric to use, then find an appropriate value of *eps* to use with DBSCAN using an appropriate evaluation method. Why did you select this distance metric? Visualise the best output found.\n",
    "</p><p>\n",
    "**HINT:** Check how many dimensions are in the data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "further_swirl_data = np.loadtxt(\"../../data/exercise_five_clusters.csv\", delimiter=\",\")\n",
    "X, true_labels= further_swirl_data[:,0:-1], further_swirl_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "# Visualise the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cluster the data and find an appropriate eps value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third and final class of clustering method we are going to look at is called *hierarchical clustering*. \n",
    "\n",
    "It is called hierarchical because the method produces a heirarchy of clusterings. \n",
    "\n",
    "At the bottom, each data point is treated as an individual cluster, at the top there is one single cluster which contains all data points. \n",
    "\n",
    "Between these two sections at different levels there are a number of clusters containing different subset of the data. We will look at this visually shortly.\n",
    "\n",
    "There are two types of hierarchical clustering available; agglomerative and divisive. \n",
    "\n",
    "An agglomerative method starts with all points being a cluster, then combining clusters one by one until we have the right number of clusters.\n",
    "\n",
    "Divisive methods start with all the data points being in one cluster, the method then breaks up the clusters until the desired number is found.\n",
    "\n",
    "In this course we will focus on agglomerative methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "Below we will go through the steps taken in an agglomerative clustering of data points.\n",
    "\n",
    "In our example we have six data points in two dimensions. We can see visually that they are roughly two groups of three data points each. \n",
    "\n",
    "**Input** \n",
    "* $X$ - training data of $n$ rows and $p$ columns. ($n$ data points, each with $p$ dimensions)\n",
    "* $j$ - number of clusters desired, $1 \\leq j \\leq n$\n",
    "\n",
    "**Step 1**\n",
    "\n",
    "Initialise all data points as their own clusters. There will be $n$ clusters at this step.\n",
    "\n",
    "<img src=\"../../images/hierc6.png\"  width=\"250\" alt=\"First step of agglom algorithm, initialising clusters at all points.\">\n",
    "\n",
    "Each data point is within a cluster of one single point.\n",
    "\n",
    "**Step 2**\n",
    "\n",
    "Calculate the distance between all the clusters. Merge the two clusters which have the smallest distance between them (most similar). There will now be one fewer clusters in the data set. \n",
    "\n",
    "<img src=\"../../images/heirc5.png\"  width=\"250\" alt=\"Combine two clusters. There are now 5 clusters, one of which contains two data points.\">\n",
    "\n",
    "We will look at how we determine the distance between clusters in the next section.\n",
    "\n",
    "**Step 3**\n",
    "\n",
    "Repeat step 2 until the number of clusters is equal to the number of desired clusters, $j$. \n",
    "\n",
    "If $j=1$ the algorithm will continue until all points are within the same cluster.\n",
    "\n",
    "\n",
    "<img src=\"../../images/heirc41.png\" alt=\"Clustering of cluster numbers 4 through 1, connecting another cluster at each step\" style=\"width: 500px;\"/>\n",
    "\n",
    "The end result of our clustering, and how well the method performs will depend on the point in the iteration that we stop combining clusters. \n",
    "\n",
    "### Number of Clusters\n",
    "\n",
    "If we stop early, with a low value of $j$ we will have many clusters, each with data points close to one another, or with data points on their own.\n",
    "\n",
    "If we have a significantly higher value of $j$ we run the risk of combining clusters that contain very dissimilar points within. \n",
    "\n",
    "As with KMeans clustering and DBSCAN, the performance of our clustering will be sensitive to the choice of parameter value used within the algorithm.\n",
    "\n",
    "A significant benefit to this method is that we can explore the model clusters at each step of it's iteration, allowing us to look at which number of clusters may be appropriate without having to re-run the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linkage\n",
    "\n",
    "Up until this point with hierarchical clustering we have been quite vague about how the distance between clusters is measured.\n",
    "\n",
    "We have discussed how the distance between points can be calculated, using a distance metric (Euclidean, Manhattan and more...), however, not how we measure the distance between groupings of points.\n",
    "\n",
    "In order to measure the distance between clusters, we need to select specific points related to each cluster, and calculate the distance using a metric between those.\n",
    "\n",
    "How the distance between clusters are determined is called a \"linkage method\". There are several methods, ranging in complexity that allow us to calculate the distance between clusters. \n",
    "\n",
    "Each linkage method lets us optimise some quality about the resulting clusters produced. This is in a similar way to the fact that different intrinsic evaluations of clustering look for different qualities in the clusters produced.\n",
    "\n",
    "We will briefly look at a few linkage methods before visualising their outputs and applying them in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Linkage and Complete-Linkage\n",
    "\n",
    "The first linkage method is an intuitive method. The distance between two clusters is defined as the distance between the two closest points between each cluster.\n",
    "\n",
    "Single linkage is computed by having the distance (with some metric) between all points in the data set, and combining the clusters which have the smallest distance between two points not in the same cluster.\n",
    "\n",
    "First we have two clusters, $A$ and $B$ where $a$ and $b$ are some point in $A$ and $B$ respectively\n",
    "\n",
    "We define the single-linkage function between two clusters as:\n",
    "\n",
    "$$ D(A,B) = \\min_{a\\epsilon A, b\\epsilon B}  d(a, b) $$\n",
    "\n",
    "$D(A,B)$ therefore gives us the closest distance between a point in A and a point in B.\n",
    "\n",
    "<img src=\"../../images/single_linkage.png\" alt=\"Single linkage, joining two clusters due to the distance between their closest elements\" style=\"width: 300px;\"/>\n",
    "\n",
    "This method is quite intuitive, join clusters that are \"near\" to each other. However, as a result of the linkage there is a tendency for long thin clusters to be produced. This may or may not be suitable for your application. \n",
    "\n",
    "We will look at this method's performance in comparison with a select few others shortly.\n",
    "\n",
    "A similarly structured and commonly used method to single-linkage is complete-linkage.\n",
    "\n",
    "With complete linkage the distance between clusters is determined using the **maximum** distance between pairs of points in separate clusters. This helps to make the clusters more compact and avoid long trailing groups. We will look at how to implement this method in the next few sections.\n",
    "\n",
    "<img src=\"../../images/complete_linkage.png\" alt=\"Single linkage, joining two clusters due to the distance between their closest elements\" style=\"width: 300px;\"/>\n",
    "\n",
    "We define the complete-linkage function between two clusters as:\n",
    "\n",
    "$$ D(A,B) = \\max_{a\\epsilon A, b\\epsilon B}  d(a, b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> do we need any example before excercise below- not a strong view on this as learning can happen through searching for solutions as well  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 6:</font></b> <p> \n",
    "Using the \"three_clusters_data\" imported below, cluster the data with $n\\_clusters=3$ using both single and complete-linkage with the AgglomerativeClustering model. The code to run the clustering is the same format as previous methods. For both results, calculate the silhouette score. Which is higher? Why do you think this is?\n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "three_clusters_data = np.loadtxt(\"../../data/clusters.csv\", delimiter=\",\")\n",
    "X, true_labels = three_clusters_data[:,0:-1], three_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average-Linkage\n",
    "\n",
    "With single-linkage we are able to use the distance between two single data points to determine whether to merge clusters. This has drawbacks are our choice in clusters is going to depend heavily on single data points, which may not reflect the collective cluster they represent. \n",
    "\n",
    "One way of reducing the impact of individual data point variance when merging clusters is to use an average of distances.\n",
    "\n",
    "Average-linkage is calculated by measuring the average distance between all points in a cluster, with all points in a different cluster. \n",
    "\n",
    "For example, in clusters $A$ and $B$, for every point in $A$ the distance is measured to every point in $B$. Using these distances the average distance between the clusters is found by dividing by the number of points in $A$ times the number of points in $B$. \n",
    "\n",
    "The sizes (number of elements within) of clusters $A$ and $B$ are given by $|A|$ and $|B|$.\n",
    "\n",
    "The clusters that are merged are then the two with the smallest average distance. \n",
    "\n",
    "$$D(A,B) = \\frac{1}{ |A| \\cdot |B|} \\sum_{a \\epsilon A}\\sum_{b \\epsilon B}{d(a,b)}$$\n",
    "\n",
    "Once the value of the average-linkage is found between all clusters in the data set, the two nearest are merged together.\n",
    "\n",
    "<img src=\"../../images/average_linkage.png\" alt=\"Average linkage, joining two clusters due to the average distance between all points in each cluster to each point in the other cluster.\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Average-linkage is also called Unweighted Pair Group Method with Arithmetic mean (UWPGMA), and has an alternative method where an weighed average is used instead of unweighted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ward-Linkage\n",
    "\n",
    "A very commonly used method for Agglomerative Clustering is Ward's method, one of the inital approaches taken in the field.\n",
    "\n",
    "To determine the Ward-linkage between clusters the variance of data points in a cluster is used.\n",
    "\n",
    "The variance $\\sigma^2$ within a group of points is the square sum of distances between each point in the group, and the mean point of that group, for $A$: centroid $\\mu_A$.\n",
    "\n",
    "$$\\sigma^2(A) = \\frac{\\sum_{a \\epsilon A}^{|A|} (a - \\mu_A)^2}{|A|} $$\n",
    "\n",
    "The Ward-linkage selects the clusters to merge which will have the minimum difference in cluster variance between original clusters, and the new merged cluster. The hypothetical cluster of A and B merged is given by $C = A \\cup B$.\n",
    "\n",
    "The difference in variance to be minimised is generally:\n",
    "\n",
    "$$\\Delta \\sigma^2(A,B) = \\sigma^2(C) - \\sigma^2(A) - \\sigma^2(B)$$\n",
    "\n",
    "Ward's distance is this difference in variance, defined more formally as:\n",
    "\n",
    "$$D(A,B) = \\frac{\\sum_{c \\epsilon C}^{|C|} (c - \\mu_C)^2}{|C|} - \\frac{\\sum_{a \\epsilon A}^{|A|} (a - \\mu_A)^2}{|A|} - \\frac{\\sum_{b \\epsilon B}^{|B|} (b - \\mu_B)^2}{|B|}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "It's important to note that to calculate the variance we are using the **squared** sum of distances. This could be the squared sum of euclidian, manhattan or other metrics. \n",
    "\n",
    "<img src=\"../../images/ward_linkage.png\" alt=\"Ward linkage, joining two clusters due to the variance of the combined clusters.\" style=\"width: 600px;\"/>\n",
    "\n",
    "The Ward-linkage is calculated for all clusters, then the two clusters with the smallest ward distance are merged, and the algorithm continues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 7:</font></b> <p> \n",
    "Why might agglomerative clustering be slow / resource demanding to implement?\n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Write your thoughts in this markdown cell.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Linkage Methods\n",
    "\n",
    "Using the code below we are going to produce clusterings based on a number of different data sets to see the differences between linkage methods.\n",
    "\n",
    "A range of data sets are used, as they each have unique properties that can be a challenge to model, however, this is still toy data designed for this purpose. \n",
    "\n",
    "You will not know what type of linkage is appropriate until you investigate your data! For this visualisation's purpose we are fixing the number of clusters within each data set, to compare each method equally.\n",
    "\n",
    "Each type of clustering algorithm on each data set is evaluated, in this case extrinsically using the Adjusted Rand Index. This checks the found group membership against the true labels. A value of 0 represents near random clustering, a value of 1; perfect labelling.\n",
    "\n",
    "*There is quite a bit of code below, it is used to produce the visualisation and does not need to be understood in depth.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up data and parameters to use\n",
    "\n",
    "# Load each data set\n",
    "circles = np.loadtxt(\"../../data/circles_simplified.csv\", delimiter=\",\")\n",
    "three_clusters_mixed = np.loadtxt(\"../../data/clusters_density.csv\", delimiter=\",\")\n",
    "three_clusters = np.loadtxt(\"../../data/clusters.csv\", delimiter=\",\")\n",
    "half_moons = np.loadtxt(\"../../data/halfmoons.csv\", delimiter=\",\")\n",
    "\n",
    "# Combine the data sets with corresponding names into lists\n",
    "data_sets = [circles, three_clusters, three_clusters_mixed, half_moons]\n",
    "data_set_names = [\"Concentric Circles\", \"Equal Density Clusters\", \"Mixed Density Clusters\", \"Half-Moons\"]\n",
    "\n",
    "# Define the sklearn appropriate linkage functions to use\n",
    "linkages = [\"single\", \"complete\", \"average\", \"ward\"]\n",
    "\n",
    "# Select the chosen metric to evaluate distances\n",
    "distance_metric = \"euclidean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Define figure and iteratively create subplots\n",
    "\n",
    "# Create the required number of subplots\n",
    "fig, ax = plt.subplots(figsize=(10, 10), ncols=len(data_sets), nrows=len(linkages))\n",
    "\n",
    "# Space the plots appropriately\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "fig.suptitle(t=\"Variety of linkage methods on diverse data patterns\", y=0.94, fontsize=14)\n",
    "\n",
    "# Iterate over the data sets, each data set is a column of the final plots\n",
    "for column_index, data in enumerate(data_sets):\n",
    "    \n",
    "    # Label the data set columns\n",
    "    ax[0, column_index].set_title(data_set_names[column_index])\n",
    "    \n",
    "    # Iterate over the linkage functions, each function is a row of the final plots\n",
    "    for row_index, linkage_function in enumerate(linkages):\n",
    "        \n",
    "        # Label the linkage rows\n",
    "        if column_index == 0:\n",
    "            ax[row_index, 0].set_ylabel(linkages[row_index], fontsize=12)\n",
    "\n",
    "        # Split the raw data between data and labels\n",
    "        X, y = data[:,:-1], data[:,-1]\n",
    "        \n",
    "        # Get the number of labels in the data set for clustering \n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Fit the clusterer on the data using the linkage function\n",
    "        clusterer = AgglomerativeClustering(n_clusters=n_labels, \n",
    "                                            linkage=linkage_function, \n",
    "                                            affinity=distance_metric).fit(X)\n",
    "        \n",
    "        # Collect the labels and score the clusterers performance\n",
    "        labels = clusterer.labels_\n",
    "        score = adjusted_rand_score(y, labels)\n",
    "        \n",
    "        # Plot the clusterers found clusters on the data\n",
    "        ax[row_index, column_index].scatter(x=X[:,0], y=X[:,1], \n",
    "                                            c=labels, cmap=\"jet\", s=12,\n",
    "                                            label=\"ARI: {}\".format(round(score, 2)))\n",
    "        \n",
    "        # Display the ARI on each plot\n",
    "        ax[row_index, column_index].legend(markerscale=0, handlelength=0, fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot going on in this figure, so we are going to break it down by data pattern. A reminder, these are **toy data sets** and representative of potential situations, investigate your data with a range of methods!\n",
    "\n",
    "**Concentric Circles**\n",
    "\n",
    "As shown in Chapter 4: Dimensionality Reduction, circular data can be a challenge to model. In clustering this is no different, many algorithms struggle to pick up the label pattern.\n",
    "\n",
    "The exception is the single-linkage method, which labels the different circles as different clusters. It is able to do this as the method favours small distances within clusters, and large gaps between different clusters. The algorithm effectively only looks nearby to merge clusters and does not care what the shape of the result looks like - perfect for this application.\n",
    "\n",
    "The complete, average and ward linkages perform poorly as demonstrated visually, and by the low ARI scores. Each favours a compact shape to separate groups, which fails when the data is not compact or linearly separable in euclidean space. Consider the average-linkage, the desired average position of both clusters is *in the same place* right in the center, making it challenging for this method to separate the groups.\n",
    "\n",
    "**Equal Density Clusters**\n",
    "\n",
    "This data pattern is a simple, three cluster data set with normally distributed groups surrounding each centre. This is one of the \"easiest\" patterns for the majority of clusters to recognise, as each group is compact, and separated. \n",
    "\n",
    "In an opposite trend to the Concentric Circles, the single-linkage method performs poorly on this data. The ARI of 0.51 is poor compared to the other methods. The clustering appears to be able to group two of the 3 clusters correctly, however, it does not pick up the third cluster as there is one single point (in green) further from a cluster than the nearest in the true third cluster.\n",
    "\n",
    "The complete, average and ward linkages perform well on this simple grouping. The only difference between the methods is one single extra point being correctly grouped in the average linkage, can you find it?\n",
    "\n",
    "This data has a relatively small variance around each centre, allowing the ward and average methods to be effective. As the data is separated well, the complete linkage is also appropriate. When we look at \"less clean\" groupings of data however, we will see that not all these linkages perform well.\n",
    "\n",
    "**Mixed Density Clusters**\n",
    "\n",
    "Compared to the equal density clusters, this data has higher variance around each center, as well as one cluster with a lower density of points. This makes hierarchical clustering, as with other methods, more challenging.\n",
    "\n",
    "Looking at the results and ARI values, it is clear the methods range from very bad (single-linkage), to okay (complete and average-linkage) up to good (ward-linkage). \n",
    "\n",
    "* Single-linkage fails to group compactly due to higher distance outliers\n",
    "* Complete-linkage determines that there are three clusters, in somewhat the right location, however, as it joins clusters with the smallest maximum cluster distance, it fails with closely grouped clusters.\n",
    "* Average-linkage locates two large clusters, with one dominating the true third labelling.\n",
    "* Ward-linkage performs well in this situation. This is because the linkage is a variance minimisation method, and the data is normally distributed.\n",
    "\n",
    "\n",
    "**Half-Moons**\n",
    "\n",
    "This data set follows a similar trend to that of the concentric circles. As the true groupings are not compact, complete, average and ward linkages fail to pick up on the two groups. \n",
    "\n",
    "Single-linkage performs well in this situation, as the clusters are separated adequately, dense and long.\n",
    "\n",
    "### Different Methods\n",
    "\n",
    "As with all modelling, there are no 100% correct answers when it comes to selecting a technique. However, as you can see above, some methods are more applicable in some situations. \n",
    "\n",
    "It's important to note that we have used relatively cleanly separated, small, low-dimensional data, which may not always be the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrograms\n",
    "\n",
    "A dendrogram is a type of visualisation that helps us to understand how our heirarchical clustering is working. It shows us which clusters are joined in what order, and what the intermediate clusters contain. \n",
    "\n",
    "On the x-axis are the data points themselves. For large numbers of data points this is sometimes instead a count of points.\n",
    "\n",
    "On the y-axis is the distance that was measured between clusters before a merge.\n",
    "\n",
    "The graph itself shows at what distance each cluster was merged. Starting at the bottom all data points separate clusters, until the agglomerative clustering is complete and the final two clusters are combined at the top link. \n",
    "\n",
    "Each merge of a two clusters is denoted by a horizontal line, a link.\n",
    "\n",
    "<img src=\"../../images/dendrogram.png\" alt=\"Ward linkage, joining two clusters due to the variance of the combined clusters.\" style=\"width: 1000px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Data**\n",
    "\n",
    "In the figure on the left we can see our data with the true class labels. What we can see from this data is:\n",
    "\n",
    "* There are 10 points in two dimensions, labelled 0-9.\n",
    "* The data points are irregularly spaced, some being closer than others.\n",
    "    * Points 3 and 6 are very close (in euclidean space!), as are 2 and 9.\n",
    "* Even if we disregard the true label values, we can see there are three natural groupings of the points.\n",
    "\n",
    "**The Graph**\n",
    "\n",
    "The dendrogram shows the result of average linkage hierarchical clustering using a euclidean metric on the data set.\n",
    "\n",
    "Starting at the bottom is each of the individual data points.\n",
    "\n",
    "> *We look at the figure from bottom to top.* \n",
    "\n",
    "* The first event that occurs is that points 3 and 6 are merged. This is because they have the smallest average linkage (single point clusters). We can see this as the distance on the y-axis where they are merged is small.\n",
    "\n",
    "* Next, further up the figure, the second nearest points 2 and 9 are merged into one cluster. Again, we can see the relatively small distance between the two (single point) clusters on the y-axis. \n",
    "\n",
    "We can count how many clusters there are at any point by \"drawing\" a line across the dendrogram, the number of intersections with the cluster lines is the number of clusters. \n",
    "\n",
    "At the bottom of the graph there are 10 intersections, as we go up there are fewer and fewer as the clusters merge. At the top we have two, then none as the final link is made, indicating there is one final cluster.\n",
    "\n",
    "Looking at the dendrogram there is a large distance (y-axis) between the merging of the red and cyan clusters. There is also a significant distance between the green cluster being merged with the other two. \n",
    "\n",
    "> This large distance between merges tells us that the clusters are not likely to be well grouped together. \n",
    "\n",
    "Along with other methods of evaluation (intrinsic and extrinsic), we can use dendrograms to analyse our clustering algorithm. Although, we should avoid using the graph to determine the number of clusters we want to use in isolation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "This course has broadly taken the approach of using one single library for all machine learning, for simicity sake, and that `sklearn` contains most algorithms required. However, hierarchical clustering and dendrogram are areas where `sklearn` is not fully equipped for our purposes.\n",
    "\n",
    "For this reason we will briefly dip our toes into the scientific computing library `scipy`. This library is *heavily* used by `sklearn`, and is maintained by the same group responsible for `numpy`. As a result, much of the syntax and data structure will be familiar. \n",
    "\n",
    "The main difference in clustering between `scipy` and `sklearn` is that `scipy` is *not* object oriented in design, we will not create a model object, fit it and predict with one class. Instead, we call functions on our data, which produce new `numpy` data. This gives us more flexibility in the processing we want to complete, but does have a slightly steeper learning curve. For our purposes in this course, this will be kept to a minimum.\n",
    "\n",
    "> As with `numpy`, I would strongly recommend becoming familiar with `scipy` as it is frequently used in python statistics and machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scipy for clustering\n",
    "\n",
    "Below we are going to implement the same clustering as we did in Exercises 1 & 6, this time using `scipy in two different ways`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "three_clusters_data = np.loadtxt(\"../../data/clusters.csv\", delimiter=\",\")\n",
    "X, labels_true = three_clusters_data[:,0:-1], three_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"True label shape:\", labels_true.shape)\n",
    "print(\"True label values:\", np.unique(labels_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we implement the single-linkage method by generating a *linkage matrix* (storage of clusters, distances and number of points at each iteration of merging). \n",
    "\n",
    "From that matrix we then determine our actually cluster labels dependent on how we want to approach this. As well as selecting how many clusters we want, we could also specify a threshold distance.\n",
    "<span style=\"color:blue\"> Can we below offer more explanation of the. code? for instance what linkage matrix look like? and what criterion='maxclust' refer to.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Returns a linkage matrix from which to derive our results\n",
    "single_linkage_matrix = linkage(X, 'single')\n",
    "\n",
    "single_linkage_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_linkage_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of the linkage matrix is as follows:\n",
    "\n",
    "* Each row of the data corresponds to a step in the interation of the clustering\n",
    "* The first and second columns `Z[i,0]`, `Z[i,1]` are indexes of clusters\n",
    "* The third column is the distance between clusters `Z[i,0]` and `Z[i,1]`. \n",
    "* The fourth column is the count of data points within `Z[i,0]` and `Z[i,1]`.\n",
    "\n",
    "There are $n-1$ rows in the linkage matrix because there are $n-1$ steps to combine $n$ data points.\n",
    "\n",
    "From this linkage matrix, created using our selected linkage method, we can find our desired clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten the linkage matrix to find the clusters, \n",
    "# In this case specify how many clusters `t` we want.\n",
    "found_single_labels = fcluster(single_linkage_matrix,\n",
    "                               criterion=\"maxclust\", \n",
    "                               t=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our linkage matrix we have all possible clusterings using the specified linkage method. We need to specify the criteria and threshold in order to produce one specific clustering. This is done by specifying the threshold value `t` (it can refere to a count, distance or other quanitities) and the criteria to measure by. In the example above `\"maxclust\"` is used. This gives us the clustering from the linkage matrix that has `t` clusters. For different options see [the documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster). A common other method is `\"distance\"`.\n",
    "\n",
    "To implement the complete-linkage method we are going to add in an intermediate step, generating a condensed distance matrix before computing the linkage. This is a more memory efficient method of storing the data, which also allows for faster clustering.\n",
    "\n",
    "Whilst not necessary for our purposes at this time, it's an interesting look behind the curtain of what is happening in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import complete\n",
    "\n",
    "# Calculate condensed distance matrix\n",
    "condensed_matrix = pdist(X, metric=\"euclidean\")\n",
    "\n",
    "# Calculate linkage matrix\n",
    "complete_linkage_matrix = complete(condensed_matrix)\n",
    "\n",
    "# Flatten linkage matrix using given criteria\n",
    "found_complete_labels = fcluster(complete_linkage_matrix,\n",
    "                                 criterion=\"maxclust\", \n",
    "                                 t=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results of the two linkage methods below, the same as shown with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4), ncols=2, sharey=True)\n",
    "\n",
    "# Plot single linkage\n",
    "ax[0].scatter(x=X[:,0], y=X[:,1], c=found_single_labels, cmap=\"jet\")\n",
    "ax[0].set_title(\"Single-Linkage\")\n",
    "# Plot complete linkage\n",
    "ax[1].scatter(x=X[:,0], y=X[:,1], c=found_complete_labels, cmap=\"jet\")\n",
    "ax[1].set_title(\"Complete-Linkage\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 8:</font></b> <p> \n",
    "Using the \"three_clusters_data\" imported below, cluster the data with $n\\_clusters=3$ use single linkage and the *scipy* package. \n",
    "</p><p>\n",
    "Calculate the silhouette score of this clustering.\n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_clusters_data = np.loadtxt(\"../../data/clusters.csv\", delimiter=\",\")\n",
    "X, labels_true = three_clusters_data[:,0:-1], three_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Dendrograms\n",
    "\n",
    "Plotting a dendrogram when we have a linkage matrix is simple. However, doing this without one, just with the `sklearn` model objects is much more challenging. \n",
    "\n",
    "> For this reason I would suggest working in `scipy` when using hierarchical clustering. Below we are going to take a subset of data and produce a dendrogram using average-linkage. \n",
    "\n",
    "We will also show how to plot many data points with this graph style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "three_clusters_data = np.loadtxt(\"../../data/clusters.csv\", delimiter=\",\")\n",
    "\n",
    "# Set a fixed random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "number_of_points = 10\n",
    "\n",
    "# Get a subset of the original data\n",
    "three_cluster_subset = three_clusters_data[np.random.choice(three_clusters_data.shape[0], \n",
    "                                                            number_of_points, \n",
    "                                                            replace=False), :]\n",
    "\n",
    "X, labels_true = three_cluster_subset[:,0:-1], three_cluster_subset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Returns a linkage matrix from which to derive our results\n",
    "average_linkage_matrix = linkage(X, 'average')\n",
    "\n",
    "# Create the dendrogram object, this is automatically plotted for us in Jupyter Notebooks\n",
    "average_dendrogram = dendrogram(average_linkage_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have more data however, we can't plot all of the leaves in our dendrogram, as that figure will have as many lines as we have rows of data. \n",
    "\n",
    "Instead, we can show the counts of data below a specified merge, the counts are shown in brackets. We call cutting off our figure \"truncating\". There are two main options to truncate the graph, by specifying a number of levels (`\"levels\"`, or the number of bottom level clusters we want shown (`\"lastp\"`).\n",
    "\n",
    "The threshold for each value is given by `p`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify a larger number of data points\n",
    "number_of_points = 200\n",
    "\n",
    "# Get a random subset of rows by generating uniform integers\n",
    "three_cluster_subset = three_clusters_data[np.random.choice(three_clusters_data.shape[0], \n",
    "                                                            number_of_points, \n",
    "                                                            replace=False), :]\n",
    "\n",
    "X, labels_true = three_cluster_subset[:,0:-1], three_cluster_subset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a linkage matrix from which to derive our results\n",
    "average_linkage_matrix = linkage(X, 'average')\n",
    "\n",
    "# Create the dendrogram object, show the counts and specify the method of truncation\n",
    "average_dendrogram = dendrogram(average_linkage_matrix, \n",
    "                                show_leaf_counts=True, \n",
    "                                p=10, \n",
    "                                truncate_mode=\"lastp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 9:</font></b> <p> \n",
    "Plot a dendrogram using the data loaded below, previously used in exercise one, and Ward linkage. \n",
    "</p><p>\n",
    "Looking at the dendrogram, what appears to be a sensible number of clusters to take?\n",
    "</p><p>\n",
    "Calculate the silhouette value of this number of clusterings.</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import and separate data\n",
    "unknown_clusters_data = np.loadtxt(\"../../data/exercise_one_clusters.csv\", delimiter=\",\")\n",
    "X, _true_labels= unknown_clusters_data[:,0:-1], unknown_clusters_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this chapter we have looked at how to perform clustering on a data set in order to understand the natural groupings. \n",
    "\n",
    "In order to first do this we explored how distances can be measured in data, looking at what defines a metric. Following from this the KMeans algorithm was worked through, showing how it groups data, then how to implement this method in `sklearn`. \n",
    "\n",
    "As with other areas of Machine Learning, the evaluation of models built is crucial, for this reason we looked at both extrinsic and intrinsic evaluation, for when we do or do not have true labels. \n",
    "\n",
    "KMeans is not the only clustering algorithm, in addition we looked at density based clustering methods and hierarchical clustering which give us flexibility to model different data distributions. Each method has benefits and good use cases, and also drawbacks. It's important to understand and get a feel for when each might be appropriate. \n",
    "\n",
    "Lastly we looked at plotting the hierarchical clustering methods, using these dendrograms to analyse the cluster structures. \n",
    "\n",
    "Much of the data we have looked at in this session has been synthetic, created to illustrate specific points. It is important to get practice with \"real\" data, especially in your domain area. Clustering is a useful analysis tool to understand data, but fundamentally the insight gained will always be dependent on the algorithm chosen.\n",
    "\n",
    "The clustering methods looked at in this chapter have all been deterministic. There are however many commonly used stochastic clustering methods, these were viewed as out of scope for this introductory level course. An outline of them and further resources are outlined in the Further Reading section.\n",
    "\n",
    "For further practice in this area work through the associated case studies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "* An alternative method for determining the number of clusters present in a data set is the *gap statistic*. [here is an implementation of this measurement](https://glowingpython.blogspot.com/2019/01/a-visual-introduction-to-gap-statistics.html) (there is not currently a major package which supports this method). The following link is to the paper which introduced the gap statistic [here](https://web.stanford.edu/~hastie/Papers/gap.pdf).\n",
    "\n",
    "* Agglomerative clustering can be improved significantly in some instances by adding conectivity contraints to the algorithm, imposing local strucure and speeding up calculations. An example on image data is given [in the sklearn documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py). [This paper discusses the impact of connectivity contraints](http://www.cs.albany.edu/~davidson/Publications/hierCameraReady.pdf)\n",
    "\n",
    "* Gaussian Mixture models are another form of unsupervised learning which can be used for clustering. An example implimentation and theory is given [on this website](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html) and here is [the sklean documentation](https://scikit-learn.org/stable/modules/mixture.html#:~:text=A%20Gaussian%20mixture%20model%20is,Gaussian%20distributions%20with%20unknown%20parameters.).\n",
    "\n",
    "* For more information and a tutorial on `scipy` have a look at their [reference guide](https://docs.scipy.org/doc/scipy/reference/).\n",
    "\n",
    "## Stochastic Methods\n",
    "\n",
    "The clustering methods we have looked at so far are deterministic in that given the same starting conditions they will return us the same clusters. This approach is bottom up, given the data and method the clusters will be found without knowledge of overall characteristics of the data.\n",
    "\n",
    "This approach can yield good results. However, we are not making any assumptions about the structure of our data.\n",
    "\n",
    "Stochastic methods, primarily Latent Class Analysis (LCA) based models allow us to leverage what we know about the distributions of our data. LCA falls under general Finite Mixure Models, which use probablistic models. Instead of measuring distances between points (clustering), the FFM models can use the probabilities of a record being a member of a class based on the distributions given.\n",
    "\n",
    "For more information on LCA please see:\n",
    "\n",
    "* [This presentation](https://hummedia.manchester.ac.uk/institutes/methods-manchester/docs/lca.pdf)\n",
    "* [This book chapter](http://www.stat.cmu.edu/~brian/720/week08/goodman-chapter.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Case study ideas\n",
    "\n",
    "Do PCA on a real data set, find appropriate method\n",
    "\n",
    "Do clustering on real data sets, find appropriate methods and optomise performance, what can be learned from the data? (do this not in a specific domain)\n",
    "\n",
    "Use PCA then cluster to get best performance (not allowed certain clustering methods), high dimension? (ie concentric circles + kernel PCA) (does this make any sense?)\n",
    "\n",
    "Add PCA to a supervised learning task, find large data set\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b><font size=\"4\"> Next Section: Case Studies X & Y</font> </b> \n",
    "<p> \n",
    "Fill in when case studies complete\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
