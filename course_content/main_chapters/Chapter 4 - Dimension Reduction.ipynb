{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<a href=\"#Lossy-Compression\"><font size=\"+0.5\">Lossy Compression</font></a>\n",
    "\n",
    "<a href=\"#Matrix-Decomposition\"><font size=\"+0.5\">Matrix Decomposition</font></a>\n",
    "* Representing Data\n",
    "* Singular Value Decomposition\n",
    "\n",
    "<a href=\"#Principal-Component-Analysis\"><font size=\"+0.5\">Principal Component Analysis</font></a>\n",
    "* Reducing to $k$ dimensions\n",
    "* Implementing PCA\n",
    "* Data Example\n",
    "\n",
    "<a href=\"#Effects-of-Dimension-Reduction-on-Models\"><font size=\"+0.5\">Effects of Dimension Reduction on Models\n",
    "</font></a>\n",
    "* Visualising Data\n",
    "* Comparing Performance in Prediction\n",
    "* Comparing Training Time\n",
    "\n",
    "<a href=\"#PCA-Variants\"><font size=\"+0.5\">PCA Variants</font></a>\n",
    "* Kernel PCA\n",
    "* Incremental PCA\n",
    "\n",
    "<a href=\"#Factor-Analysis\"><font size=\"+0.5\">Factor Analysis</font></a>\n",
    "* Further Factor Analysis Methods\n",
    "\n",
    "<a href=\"#Dimension Reduction for Visualisation\"><font size=\"+0.5\">Dimension Reduction for Visualisation</font></a>\n",
    "* t-SNE\n",
    "\n",
    "<a href=\"#Summary\"><font size=\"+0.5\">Summary</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><font size=6>Unsupervised Learning</font></h1></center>\n",
    "\n",
    "The next two chapters focus on unsupervised learning, a field of machine learning. Unsupervised learning happens when we do not have labels for our data, in that there is no target value. Because there are no labels to \"supervise\" the learning we need to gather insight from the data without labels, which is where methods in unsupervised learning come in. Unsupervised learning is a broad topic. In this course we will just cover two main areas: dimension reduction and clustering.\n",
    "\n",
    "There is a considerable amount of technical backend to the topics discussed in these chapters. We will provide the minimum theory / mathematics relevant but would suggest completing the ML Theory course for a more comprehensive exploration.\n",
    "\n",
    "\n",
    "<center><h1><font size=7>Dimension Reduction</h1></center>\n",
    "\n",
    "Data Dimension Reduction is an Unsupervised Learning technique; here, the aim is not predicting a target variable and we do not have our data labelled, instead we aim to reduce the size of our data to the managable scale for further analysis. The information within this chapter will be a useful supplement to the Data Preparation section of Introduction to Machine Learning. \n",
    "\n",
    "As our data gets larger in number of features, we will often struggle to determine the signal from the noise. With more features given to use, it can be hard to decide which are important to our machine learning models, and how we can show these features; furthermore, are features can be highly correlated and it is hard to detect potential multicolinearity. With a greater scale of data, we might start to ask questions that haven't been tackled in the previous machine learning sections, these could be:\n",
    "\n",
    "* How to make our models run faster and better?\n",
    "* How can I illustrate this complex data?\n",
    "* What are the most important parts of this data?\n",
    "\n",
    "How to visualise and manipulate data of many dimensions is tackled using an area of Unsupervised Learning called *Dimension Reduction*. This topic focuses on reducing the number of features in our data by *combining* them in a way which maximises the amount of information contained. This recreates our data with new, fewer features, which we can then work with. The reduced features can then be incorporated into other machine learning algorithms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "By the end of this course you should be able to:\n",
    "\n",
    "* Understand what dimension reduction is.\n",
    "* Understand the effect of dimension reduction techniques.\n",
    "* Be aware of the benefits and difficulties of using these techniques.\n",
    "* Be aware of how Principle Component Analysis works.\n",
    "* Be able to implement dimension reduction in python for machine learning.\n",
    "* Be aware of how t-SNE works and when to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lossy Compression\n",
    "\n",
    "What is dimension reduction? In our machine learning context, it is the reduction of the number of features (columns) of our dataset, whilst minimising the loss of information.\n",
    "\n",
    "This generally falls under the topic of \"Lossy Compression\".\n",
    "\n",
    "The *Compression* part shows that we are reducing the **quantity** of total data that we have in our data set.\n",
    "\n",
    "The *Lossy* part denotes that we are going to inherently **lose information** by reducing the dimensions of our data. Our goal for good dimension reduction is to *minimise the amount of **useful** information lost*. \n",
    "\n",
    "How we mathematically implement dimension reduction will be covered in later sections. This section will highlight the characteristics of general dimension reduction. \n",
    "\n",
    "For a more in depth exploration and further mathematics - see the ML Theory course.\n",
    "\n",
    "Our original data set will be denoted by $X$ throughout, we presume that all of our data is numerical.\n",
    "\n",
    "$X$ has several qualities that are useful for our machine learning models. These include information such as:\n",
    "\n",
    "* Distributions of data (ranges, shapes, variances, averages and more)\n",
    "* Correlations/covariance between features\n",
    "\n",
    "We can convert our different features into a new set of features, each of these new features is some combination of our original features. For now this will be called **encoding** our data. \n",
    "\n",
    "Our goal is to transform our data from one set of features to another smaller number of features. \n",
    "\n",
    "We will denote encoding our data using the function $f(X)$ which will produce a new, smaller data set $X'$.\n",
    "\n",
    "$$f(X) \\rightarrow X'$$\n",
    "\n",
    "We now have a new, dimension reduced data set to work with. \n",
    "\n",
    "However, just reducing the dimensions does not inherently mean that we have a *good* new data set. We could be removing the most important columns to make the data smaller!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/encode.png\"  width=\"500\" height=\"500\" alt=\"Visual representation of encoding step of lossy compresison.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce a *good* dimension reduction, we need to know *how much information has been lost* in the encoding. To do this we introduce a **decoder** function $g(X')$ which will transform our reduced dimension data into the original number of dimensions. This is done because we cannot compare data of different dimensions directly.\n",
    "\n",
    "Where $f(X)$ reduces the number of dimensions, $g(X')$ increases them back to the original size.\n",
    "\n",
    "Consider it as a forward then backward transformation. \n",
    "\n",
    "Or in a more \"real-world\" situation: heating a sauce ($f(X)$ encoding) to reduce the amount of water in it, concentrating the flavour (information). \n",
    "\n",
    "By this analogy we add water back to the sauce to bring it back to its original concentration ($g(X')$ decoding).\n",
    "\n",
    "We show the new data as $X''$.\n",
    "\n",
    "$$g(X') \\rightarrow X''$$\n",
    "\n",
    "$$\\therefore~~ g(f(X)) \\rightarrow X''$$\n",
    "\n",
    "This data that has been decoded is in the original number of dimensions, but crucially; the data is *not the same as the original data*. By encoding and decoding our data using dimension reduction we have **lost information**.\n",
    "\n",
    "Using our cooking analogy; the decoded sauce will be the same concentration as the original sauce, but it won't taste the same as we have gone through an irreversible process (heating/encoding).\n",
    "\n",
    "*This analogy is of course a bit of a stretch, but should be clearer as we look into actual implementations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/decode.png\"  width=\"700\" height=\"500\" alt=\"Visual representation of encoding then decoding step of lossy compresison.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two data sets of the same dimension $X$ and $X''$ we can compare them. $X$ will have more useful information than $X''$.\n",
    "\n",
    "Our goal in dimension reduction is **to reduce the information difference between our original data and decoded data**. As a pseudo equation this means we are trying to:\n",
    "\n",
    "$$ minimise( ~ info(X) - info(X'') ~) $$\n",
    "\n",
    "Our task involves **finding the best functions $f(X)$ and $g(X')$ to encode and decode our data into a smaller dimension and back**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/infoquant.png\"  width=\"500\" height=\"450\" alt=\"Visual representation of the difference in information between X and X''. X >= X''.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Decomposition\n",
    "### An Introduction\n",
    "\n",
    "Some methods of dimension reduction rely on a process called Singular Value Decomposition, this is a method of linear algebra which allows us to change the dimensions of matrices. We are going to cover this topic at a high level so the *intuition* of the method is understood. An in-depth understanding of linear algebra is not needed for this course.\n",
    "\n",
    "Before we look into decomposing matrices we are first going to look at how we can represent our data in a more mathematical format.\n",
    "\n",
    "## Representing Data\n",
    "\n",
    "So far we have looked at our data in tables using `pandas` and then briefly in `numpy` arrays. These are data structures that contain our elements of data, what we need is a way to represent our data in a way we can mathematically work with.\n",
    "\n",
    "We can represent a column in our data as a vector $x$, with $n$ rows.\n",
    "\n",
    "The elements of the vector correspond to each of our data points in the column.\n",
    "$$ x = \n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "...\\\\\n",
    "x_n\\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Vectors allow us to represent directions in space.\n",
    "\n",
    "We can convert our vector from vertical to horizontal (column to a row) using a matrix transposition which is denoted by a $^T$ after our vector.\n",
    "\n",
    "The transpose of $x$ is therefore:\n",
    "$$ x^T = \n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "...\\\\\n",
    "x_n\\\\\n",
    "\\end{array}\n",
    "\\right)^T\n",
    "= \n",
    "\\left(\n",
    "\\begin{array}{r}\n",
    "x_1~~x_2~~...~~x_n\\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Our data of many columns can be represented as multiple vectors put together into a matrix. If we have $p$ vectors with $n$ elements we can concatenate them to a matrix $X$ with $n$ rows and $p$ columns.\n",
    "\n",
    "Each element in the matrix is given by the indexes $i$ and $j$, the row and column number respectively. The element in the first row, second column is therefore $X_{1,2}$.\n",
    "\n",
    "Our matrix then takes the form:\n",
    "\n",
    "$$ X = \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "x_{1,1}~~x_{1,2}~~...~~x_{1,p}\\\\\n",
    "x_{2,1}~~x_{2,2}~~...~~x_{2,p}\\\\\n",
    "x_{3,1}~~x_{3,2}~~...~~x_{3,p}\\\\\n",
    "...~~...~~...~~...\\\\\n",
    "x_{n,1}~~x_{n,2}~~...~~x_{n,p}\\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "We can now apply functions to our matrix to manipulate our data.\n",
    "\n",
    "Having multiple vectors allows us to represent not just a direction in space, but with two vectors a plane, and more than two a volume.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "We can think of performing SVD on our data as untangling the relationships between our features so our new columns are not correlated with each other. We transform data into a new space based on principal components (eigen vectors). We are maximising the variance along dimensions that are mostly not correlated. \n",
    "\n",
    "For Singular Value Decomposition (SVD) to work we are going to assume our data is centered. This means that the mean of each column's values is zero for all columns. This is achieved by subtracting the columns mean from all values in the column, in a similar manner to how we scaled data in Chapter 1: Data Preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of performing SVD on a data set with two features. We convert the two features $x_1$ and $x_2$ into two new dimensions (component 1 & 2) by rotating and scaling our data. The components found by the decomposition are shown on both plots. If we had data of more than two features, we could reduce those dimensions down to two in order to plot them.\n",
    "\n",
    "The most important dimension is given by the longest vector. \n",
    "\n",
    "<img src=\"../../images/SVDexample.png\"  width=\"1200\" height=\"600\" alt=\"Visual representation matplotlib example of decomposition of real data.\">\n",
    "Figure A.\n",
    "\n",
    "How this graph was created is shown in the `extras` folder under \"SVD Diagram Production.ipynb\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have discussed previously that our data contains relationships between the variables (correlations, covariances).\n",
    "\n",
    "As a result, we can express the data matrix values in a different format using the relationships between columns. \n",
    "\n",
    "The new format is achieved by rotating and scaling the data into its *component parts* - [eigenvectors](https://www.mathsisfun.com/algebra/eigenvalue.html). \n",
    "\n",
    "This is called a *matrix decomposition*; we are decomposing the original matrix into a different representation: three new matrices. \n",
    "\n",
    "When multiplied together the matrices give the original data $X$.\n",
    "\n",
    "$$X = USV^T$$\n",
    "\n",
    "We now have three new matrices $U$, $S$ and $V^T$, note that $V^T$ is V transposed.\n",
    "\n",
    "Our original data $X$ was organised by columns (vectors), our new matrices represent a transformation into new dimensions. \n",
    "\n",
    "The new representation of our data is in the plane (component axis) that give us the largest variance possible. \n",
    "\n",
    "In the below diagrams we need to keep in mind that both sides of the equals sign are **equivalent**. On the left we have our initial data. On the right we have a decomposition of the original data, that has rotations and scaling. \n",
    "\n",
    "* **U** rotates our data into a new set of dimensions\n",
    "* **S** scales our data to its new values in the new dimensions (a diagonal matrix)\n",
    "* *$V^T$* rotates the data back to the original dimensions\n",
    "\n",
    "We can later inspect the matricies **U**, **S** and *$V^T$* to understand our data and the importance of different features.\n",
    "\n",
    "<img src=\"../../images/SVD.png\"  width=\"700\" height=\"450\" alt=\"Visual representation the matrix decomposition of X using SVD\">\n",
    "\n",
    "It's important to note that these new dimensions created will not correspond directly to our original columns. Some of our new dimensions are **more important** to represent the variance than others. \n",
    "\n",
    "Each principal component axis has a special property relating them to each other: they are **orthogonal**. This means that they are all at right angles to each other and therefore do not interact. \n",
    "\n",
    "<br>\n",
    "\n",
    "This is a difficult topic that takes time, and application to understand. \n",
    "\n",
    "<br>\n",
    "\n",
    "We will now look at why decomposing our data into three matrices is actually useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is a method used to reduce the number of dimensions in a matrix, this relies on SVD. \n",
    "\n",
    "The \"Principle Components\" are the new columns (dimensions) after rotating our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing to $k$ dimensions\n",
    "\n",
    "When we perform SVD and rotate our data inline with dimensions which maximised the variations in our data,  we get our columns of $U$ ordered by how much variance of the original data they explain. The scale of this is given by the elements in $S$. \n",
    "\n",
    "The \"principle components\" are the new dimensions/features after rotating our data. The rotated dataset in the direction of principal components is given by $US$.\n",
    "\n",
    "Our goal is to reduce the dimensions of our dataset. We only want the $k$ most important dimensions and can discard the rest. \n",
    "\n",
    "Because our data is ordered by importance (the variance in each dimension), we keep only the first $k$ columns of $U$ and first $k$ rows and columns of $S$.\n",
    "\n",
    "**This new matrix $X' = U'S'$ is the reduced dimension data set that we can now use in our machine learning models**\n",
    "\n",
    "<img src=\"../../images/PCA.png\"  width=\"700\" height=\"450\" alt=\"Visual representation getting X' by using the first k columns of U'S'\">\n",
    "\n",
    "The decomposition and subsequent removal of less important principle components can be considered equivalent to our function:\n",
    "\n",
    "$$f(X) \\rightarrow X'$$\n",
    "\n",
    "To transform our data back to the original dimensions we multiply:\n",
    "\n",
    "$U'S'$ by $V^T$'. Where $V^T$' is the first $k$ rows of $V^T$.\n",
    "\n",
    "$$ X'' = U'S' V^{T}{'} $$\n",
    "\n",
    "<img src=\"../../images/reconstruct.png\"  width=\"600\" height=\"450\" alt=\"Visual representation getting X'' by using the first k columns of U'S' then multiplying by V^T'\">\n",
    "\n",
    "<br>\n",
    "This is equivalent to the function $g(X')$ discussed in the previous section.\n",
    "\n",
    "$$g(X') \\rightarrow X''$$\n",
    "\n",
    "The difference between $X''$ and $X$ gives us the *quality of the dimension reduction process*.\n",
    "\n",
    "Each principal component will **explain some amount of the variance of the original data**. The first component will explain the largest amount, then decreasing from there. \n",
    "\n",
    "If we take $k = p$ (new columns = old columns), the components of our decomposition will (in theory) explain all the variance of the original data.\n",
    "\n",
    "When we take $k~<~p$ then the explained variance of our new reduced data will be **less** than that of the full decomposition. \n",
    "\n",
    "**By removing principle components, we remove some explained variance. However, by removing the components in order of least to most important we can still explain much of the variance.**\n",
    "\n",
    "We will use a ratio of how much variance is explained by our new data when compared to using all principle components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, we **do not** need to directly implement this matrix decomposition ourselves. As with many machine learning topics, there is class within `sklearn` that performs the necessary steps to decompose our data and remove the specified number of components.\n",
    "\n",
    "To perform PCA our data needs to be properly scaled, which `sklearn` handles itself.\n",
    "\n",
    "The most important hyperparameter for using PCA is the **value of $k$, how many components we want.** \n",
    "\n",
    "As with all hyperparameters in `sklearn` you can optimise a model with hyperparameter tuning.\n",
    "\n",
    "Instead of using the notation of $k$, `sklearn` has the parameter `n_components` which describes the same thing; how many components we want our final data to have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Example\n",
    "\n",
    "We are going to use a new data set called `high_dimension`. This was a data set created to practice classification problems with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_dim = np.loadtxt(\"../../data/high_dimensions.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final column of our data set is the target column `y`.\n",
    "\n",
    "We will be using data throughout this course that has been stored with features `X` and targets `y` in the same file. Given `n` columns we will take the `n`th column as `y` and the `n` to `n-1` columns as `X`. We will then reduce the dimensionality of `X` and explore any relationships with `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_dim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the data set\n",
    "X = high_dim[:,0:-1]\n",
    "y = high_dim[:,-1]\n",
    "unique_y, y_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Unique values of y: \", unique_y)\n",
    "print(\"Counts of y: \", y_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target values in `y` are $0$ and $1$, with counts of $400$ and $600$ respectively.\n",
    "\n",
    "We now have a data set `X` of 1000 rows and 100 features. \n",
    "\n",
    "* $n = 1000$\n",
    "* $p = 100$\n",
    "\n",
    "This is clearly too many for us to plot or analyse one by one. We are going to reduce the number of dimensions to $k=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "# Create PCA object\n",
    "pca = PCA(n_components=k, random_state=123)\n",
    "\n",
    "# Fit the PCA object to the data (performs SVD and reduce dimensions)\n",
    "pca.fit(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform the data into reduced dimensions (Remove components)\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new data `X_reduced` has $n$ rows and $k$ dimensions. \n",
    "\n",
    "From the `pca` object we can get characteristic properties of the components.\n",
    "\n",
    "Firstly we get the principle components found themselves, given by `pca.components_`, these are vectors in our original feature space. The vectors represent a direction of maximum variance. While interesting, they aren't hugely useful to us right now.\n",
    "\n",
    "Each of our 5 components will contain 100 values. Each value in the component corresponds to the projection of the original feature column to the new principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shape of resulting matrix\n",
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shape of single component\n",
    "pca.components_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first component of the PCA in the original data space\n",
    "pca.components_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the explained variance of each component, given by `pca.explained_variance_`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the first component explains more variance than the second and so on.\n",
    "\n",
    "However, it is more useful for us to look at the **explained variance ratio** of each component.\n",
    "\n",
    "This will help us judge how useful each component is at explaining the variance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evr = pca.explained_variance_ratio_\n",
    "\n",
    "# Produce a percentage\n",
    "evr_pct = evr*100\n",
    "\n",
    "print(\"The explained variance of the first component is: {} %\".format(evr_pct[0].round(2)))\n",
    "print(\"The explained variance of the second component is: {} %\".format(evr_pct[1].round(2)))\n",
    "print(\"The total explained variance of all {} components are: {} %\".format(k, evr_pct.sum().round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that our first component explains some amount of the variance, and all of the $k=5$ components explain the more of the variance, but information about the original data is clearly lost!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 1:</font></b> <p> \n",
    "Perform PCA again on $X$, this time with $n\\_components=10$, what is the difference in **total** explained variance between using five and ten components? Which number of components might we prefer to use?\n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of Dimension Reduction on Models\n",
    "\n",
    "We are now able to perform PCA on our data set to make new data using principle components. We will now explore what *impact* this has in a supervised machine learning context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising data\n",
    "\n",
    "We are first going to look at the new data in two dimensions. Using our target attribute, we can visualise how our classification model might learn relationships in the new data. Visualising all the data at once is *something we would not be able to do* with our original data.\n",
    "\n",
    "It's important to note that we have not used `y` yet to train anything, the visualisation shows the *natural spread of the classes using only* `X`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "\n",
    "# Create the new PCA object with k=2\n",
    "pca = PCA(n_components=k)\n",
    "\n",
    "# Transform the data into two dimensions\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Total variance ratio explained by {} components: {}%\".format(k, (pca.explained_variance_ratio_.sum()*100).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_dimension = X_pca[:,0]\n",
    "second_dimension = X_pca[:,1]\n",
    "\n",
    "# Plotting data in new dimensions\n",
    "# Adding the colour as the class labels lets us see\n",
    "# how the components correspond to target class values\n",
    "plt.scatter(x=first_dimension, y=second_dimension, c=y, alpha=0.2, cmap=\"cool\")\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that based on the target classes the two dimension projection of our data could help predict values as there seems to be some separation occuring. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Performance in Prediction\n",
    "\n",
    "We are going to use a simple, untuned Logistic Regression model to compare how well our reduced dimension data performs compared to the original set.\n",
    "\n",
    "This is done using the F1 score as our metric with training and test splits.\n",
    "\n",
    "First the data needs to be centred for fair comparison as PCA does this for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Find the mean of each column used for centring\n",
    "column_means = np.mean(X, axis=0)\n",
    "\n",
    "# Center the data\n",
    "# This helps the PCA perform better\n",
    "X_centred = X - column_means\n",
    "\n",
    "# Split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_centred, \n",
    "                                                    y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "original_model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = original_model.predict(X_test)\n",
    "\n",
    "# Measure performance\n",
    "original_score = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Original data score:\", original_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make a function that does the PCA, model training and prediction steps for us given the input data and number of desired dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA_train_predict_score(X, y, k, random_state=42):\n",
    "    \"\"\"\n",
    "    Function to perform PCA, generate Logistic Regression model \n",
    "    evaluating performance of given k components.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array (n,p)\n",
    "        Feature data\n",
    "    y : np.array (n,)\n",
    "        Target data\n",
    "    k : int \n",
    "        Number of components for PCA to return\n",
    "    random_state : int\n",
    "        Random seed for train_test_split\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        f1_score result of model trained.\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=0.33, \n",
    "                                                        random_state=random_state)\n",
    "    # Fit the PCA model on the training data\n",
    "    pca = PCA(n_components=k).fit(X_train)\n",
    "    \n",
    "    # Apply our learned PCA transformation from the test data on the \n",
    "    # training and test sets.\n",
    "    # This puts the test data into the same component factor space as\n",
    "    # the trained data\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    # Train the model \n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "    \n",
    "    # Generate predictions and score the predictions \n",
    "    # Using f1 score described in Chapter 3\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder - the `f1` score is calculated to measure the performance of prediction using:\n",
    "\n",
    "$$ F1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} $$\n",
    "\n",
    "or \n",
    "\n",
    "$$ F1 = \\frac{TP}{TP + \\frac{1}{2} (FP + FN)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The maximum number of components is given by the original feature number\n",
    "k_max = X.shape[1]\n",
    "\n",
    "# Create a list of k_values\n",
    "k_values = list(range(1, k_max + 1))\n",
    "\n",
    "# Generate F1 scores for each k value of PCA\n",
    "f1_scores = [PCA_train_predict_score(X, y, k) for k in k_values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot and compare the resulting performance of different values of k\n",
    "plt.title(\"F1 score over all values of k\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.ylim(0.75, 0.88)\n",
    "\n",
    "plt.hlines(y=original_score, xmin=-1, xmax=k_max, label=\"No-PCA\", colors=\"grey\", linestyles=\"dotted\")\n",
    "\n",
    "plt.scatter(x=k_values, y=f1_scores, c=f1_scores)\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above data, that choosing to reduce our dimensionality can *improve* our model's predictive performance in some cases. This is because it can remove unimportant information in the original data set: statistical noise.\n",
    "\n",
    "There are three trends here to note:\n",
    "\n",
    "* Starting from $k = 1$ the `f1` performance **increases** with increasing $k$. Adding more significant dimensions gives our model more information to train on, producing a better score.\n",
    "* There is a peak somewhere between $k=5 - 20$. Above this amount our performance gradually declines. This is because our model is overfit to the training data. It is creating principle components using noise not real trends - this propogates through to the model to reduce performance.\n",
    "* At a higher number of $k$ adding more components **decreases** the model's performance more significantly, as we tend towards $k \\rightarrow 100$ the performance of our model gets closer to the value of the model trained without PCA. \n",
    "\n",
    "\n",
    "\n",
    "From this we can see that selecting the right value of $k$ can **significantly** (~5%) improve our model's predictive performance.\n",
    "\n",
    "To best determine an optimal value of $k$ we could perform a cross validation for a range of values of $k$. We could then compare the performance of the training and test `f1` scores. We could then choose the value of $k$ with the best test `f1` scores. With the best $k$ value found we would then continue our modelling process.\n",
    "\n",
    "The method shown in the figure above is a more simplistic version of the cross validation approach. The cross validation approach will provide a more statistically robust result. Instead of using `train_test_split` you could instead use `sklearn.model_selection.cross_validate` or related methods.\n",
    "\n",
    "<br>\n",
    "\n",
    "Side note: the \"true\" number of components used to create this data set was $k=10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 2:</font></b> <p> \n",
    "What value of $k$ has the highest associated F1 score? What does that tell you about our data?\n",
    "</p><p>\n",
    "**HINT** Look up the numpy function $np.argmax()$\n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Training Time\n",
    "\n",
    "As the size of our data gets large, the time taken to train a model increases drastically. One consideration for how many dimensions of data to choose is how long will it take to train our model. We often have trade-offs between accuracy, time and memory contraints in computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to modify our `PCA_train_predict_score()` function to measure how long it takes to train a model in addition to the F1 score. We can then compare each model. In this example we are going to just look at how long the model training takes, not how long the whole processing takes.\n",
    "\n",
    "*We are using a relatively small data set, so the scale of times may be small*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def PCA_train_predict_score_time(X, y, k, random_state=42):\n",
    "    \"\"\"\n",
    "    Function to perform PCA and generate model to check model performance\n",
    "    and training time of given k components.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array (n,p)\n",
    "        Feature data\n",
    "    y : np.array (n,)\n",
    "        Target data\n",
    "    k : int \n",
    "        Number of components\n",
    "    random_state : int\n",
    "        Random seed for train_test_split\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        f1_score result of model trained.\n",
    "    training_time : float\n",
    "        The time taken to train the model\n",
    "    \"\"\"\n",
    "    # Split the data \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=0.33, \n",
    "                                                        random_state=random_state)\n",
    "    # Fit the PCA to the training data\n",
    "    pca = PCA(n_components=k).fit(X_train)\n",
    "    \n",
    "    # Our test data needs to be PCA'd on the original PCA model fit\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    # Create the model object\n",
    "    estimator = LogisticRegression()\n",
    "    \n",
    "    # Get the time at the start and end of the model training\n",
    "    start_time = time.time()\n",
    "    model = estimator.fit(X_train, y_train)\n",
    "    finish_time = time.time()\n",
    "    \n",
    "    # Generate predictions and score\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate total training time\n",
    "    training_time = finish_time - start_time\n",
    "    \n",
    "    return score, training_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the time and performance data for each value of $k$ we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The maximum number of components is given by the original feature number\n",
    "k_max = X.shape[1]\n",
    "\n",
    "# Create a list of k_values\n",
    "k_values = list(range(1, k_max + 1))\n",
    "\n",
    "# Generate F1 scores and model training time for each k value of PCA\n",
    "f1_scores_times = [PCA_train_predict_score_time(X, y, k) for k in k_values]\n",
    "\n",
    "# Separate the list of tuples into two lists\n",
    "f1_scores, times = zip(*f1_scores_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting our non-PCA performance to compare with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_centred, \n",
    "                                                    y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Create the model object\n",
    "original_estimator = LogisticRegression()\n",
    "\n",
    "# Get the time at the start and end of the model training\n",
    "start_time = time.time()\n",
    "original_model = original_estimator.fit(X_train, y_train)\n",
    "finish_time = time.time()\n",
    "\n",
    "# Generate predictions and evalute the performance\n",
    "y_pred = original_model.predict(X_test)\n",
    "original_score = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the time taken to train\n",
    "original_time = finish_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results of the measurements of time taken to train shows that the fewer dimensions used to train the data the faster the model trains.\n",
    "\n",
    "However, we can see that the *fastest* models to train are **not** the *best performing* models.\n",
    "\n",
    "There is a trade-off between number of dimensions reduced to, the speed of training and the model's final performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(k_values, times, c=f1_scores)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"F1 Score\")\n",
    "plt.title(\"Model training time over all values of k\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"Training time (s)\")\n",
    "plt.ylim(0, 0.08)\n",
    "plt.hlines(y=original_time, xmin=-1, xmax=101, label=\"Without PCA\", colors=\"grey\", linestyles=\"dotted\")\n",
    "plt.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the lowest $k$ have the fastest speed, but low performance. Higher $k$ improve the model performance and are faster than the original model, however increasing $k$ too much decreases performance and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 3:</font></b> <p> \n",
    "Using $X$ fit a PCA model with $k = 50$.\n",
    "\n",
    "Plot the cumulative sum of variance explained ratio of each component.\n",
    "</p>\n",
    "<p>\n",
    "**HINT:** use the numpy $np.cumsum()$ function\n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PCA Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernal PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA method discussed so far creates a set of dimensions that are orthogonal to each other (at right angles). This is a good assumption for some data; however this linear model will not capture important aspects of all data.\n",
    "\n",
    "The relationships between some features may not be linear, by using Kernel PCA we can capture this non-linear behaviour.\n",
    "\n",
    "One common example of non-linear data properties that linear PCA cannot adequately express is circularly separated data.\n",
    "\n",
    "An example of this is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in our concentric circle data\n",
    "circle_data = np.loadtxt(\"../../data/circles.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split X and Y data\n",
    "X_circles, y_circles = circle_data[:,0:-1], circle_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set we have three columns, the first two are the continuous data features. The third is a binary target attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(\"Original Dimensions of Features\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.scatter(x=X_circles[:,0], y=X_circles[:,1], c=y_circles, cmap=\"coolwarm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 4:</font></b> <p> \n",
    "Using $X\\_circles$ perform *linear* PCA with $k = 2$. Plot the resulting new dimensions of data. Calculate the **total** explained variance ratio of the model and discuss why it may be difficult for a linear machine learning model to learn the pattern of this data.\n",
    "\n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that this will reduce the dimensions of our data, but this new representation is similar to the original. Using a linear PCA doesn't separate our two classes in an intuitive way. \n",
    "\n",
    "Using Kernel PCA we can use the non-linear structure of the data to find the best components.\n",
    "\n",
    "We have multiple options for the choice of kernel that is most appropriate, these include:\n",
    "\n",
    "* \"linear\" - fits a linear kernel\n",
    "* \"poly\" - fits a polynomial (function of order > 1) function\n",
    "* \"rbf\" - fits a radial basis function kernel\n",
    "* and more!\n",
    "\n",
    "For more information on kernels and how `sklearn` defines them check [here](https://scikit-learn.org/stable/modules/metrics.html#metrics).\n",
    "\n",
    "Our data is in concentric circles, so we are going to choose the **radial basis function** as it can represent circles.\n",
    "\n",
    "The parameter `gamma` is the kernel coefficient, which determines the strength of the kernel function's effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=2)\n",
    "\n",
    "X_kpca = kpca.fit_transform(X_circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(\"New dimensions using rbf kernel\")\n",
    "plt.xlabel(\"$component~1$\")\n",
    "plt.ylabel(\"$component~2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.scatter(x=X_kpca[:,0], y=X_kpca[:,1], c=y_circles, cmap=\"coolwarm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes the difference between the two concentric circles in the original data clearer in the new dimensions, they are better separated.\n",
    "\n",
    "Our machine learning model would find it easier to learn the difference between the classes in this case rather than when the data was in circles.\n",
    "\n",
    "A really simple model, such as defining the predicted class purely based on the value of `component 1` would perform very well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 5:</font></b> <p> \n",
    "Perform Kernel PCA on the $X\\_circles$ data set using the \"rbf\" kernel and $k = 2$.\n",
    "</p><p>\n",
    "Use values of $gamma=1$ and $gamma=7$. What effect does this have?\n",
    "</p><p>\n",
    "Plot the results of each value.\n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental PCA\n",
    "\n",
    "PCA is a very powerful tool in reducing the size of our data. However, traditional methods for calculating the SVD of a data set require all the data to be held within the computer memory at once. \n",
    "\n",
    "This can be impractical for data sets which are larger than our computer memory. \n",
    "\n",
    "In this case we can use Incremental PCA, which partially fits the PCA over batches of data samples. This means we are only fitting to the amount of data in one single batch of data at any one time, reducing the size of memory needed. \n",
    "\n",
    "The `sklearn` implementation of this method by default sets the `batch_size= 5 * n_features`.\n",
    "\n",
    "This method will produce different, but similar principle components to linear PCA.\n",
    "\n",
    "*This example uses data that would fit in the computer memory as a toy example.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 6:</font></b> <p> \n",
    "Using $ipca\\_20$ fit the data to $X$. Compare the total variance explained ratio of $ipca$ with the original PCA method using $k=20$.\n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "ipca_20 = IncrementalPCA(n_components=20)\n",
    "\n",
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis\n",
    "\n",
    "PCA and it's variants are one subset of dimension reduction for analysis methods.\n",
    "\n",
    "When performing dimension reduction techniques we map our data from it's original space to a new space that represents combinations of features. These combinations of features have meaning as latent variables. We have different approaches to the mapping / combining of features. By choosing different methods we can interpret our data in different ways.\n",
    "\n",
    "Another approach, which is similar to PCA, but with key difference is Factor Analysis. Instead of principal components as our resulting space to analse we instead have factors.\n",
    "\n",
    "The classical PCA we looked at first has the following characteristics:\n",
    "\n",
    "* Data is continuous\n",
    "* Components are orthogonal / not correlated\n",
    "\n",
    "Approaches in Factor Analyis allow us to use different data types other than continuous variables, and let us analyse latent variables in different ways. \n",
    "\n",
    "Our features are a combination of weighted factors. We can specify how many factors we want to find in our data \n",
    "\n",
    "Consider a data set $X$ with $p$ features/variables $X_1, X_2, ..., X_p$.\n",
    "\n",
    "After performing Factor Analysis we find $m$ factors, where $m < p$ (remember we are trying to reduce our dimensions!) $F_1, F_2, ..., F_m$.\n",
    "\n",
    "Each feature $X_i$ is: $$X_i = a_{i1}F_1 + a_{i2}F_2 + ... +a_{im}F_m + e_i $$\n",
    "\n",
    "Where $a_i$'s are the factor score - explaining how important each factor is in explaining the feature. The value $e_i$ is the error that cannot be explained by a factor.\n",
    "\n",
    "Some factors will explain some features well, and have high factor scores. Some will have low factor scores, meaning the factor does not explain the feature well. The goal of factor analysis is to separate out features such that combinations of features are explained by separate factors.\n",
    "\n",
    "Factor Analysis can be performed in a similar way to PCA using `sklearn.decomposition.FactorAnalysis`. This method of factor analysis uses maximum likelihood estimate to build the factors - and as such its performance can be evaluated using a log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "high_dim = np.loadtxt(\"../../data/high_dimensions.csv\", delimiter=\",\")\n",
    "\n",
    "X, y = high_dim[:,0:-1], high_dim[:,-1]\n",
    "\n",
    "# Create 10 factors to analyse\n",
    "fa = FactorAnalysis(n_components=10).fit(X)\n",
    "\n",
    "factors = fa.transform(X)\n",
    "\n",
    "# look at first factor\n",
    "factors[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Factor Analysis\n",
    "\n",
    "The above basic method of Factor Analysis can be very powerful. It is often better to use factor analysis for exploratory data analysis. Factor Analysis assumes that the noise in each feature is gaussian, but not the same distribution for each feature.\n",
    "\n",
    "The `sklearn` implimentation above is used on continuous data. However, we will often instead want to work with  categorical data instead (ordinal or nominal). There are other methods of FA that can be used in different data type situations.\n",
    "\n",
    "These are beyond the scope of this course, but added for reference.\n",
    "\n",
    "* Using a contingency table (cross-tab or pivot): Correspondence Analysis (CA)\n",
    "* More than 2 features that are all categorical: Multiple Correspondence Analysis (MCA)\n",
    "* Groups of Categorical *or* numerical features, not both: Multiple Factor Analysis (MFA)\n",
    "* Both categorical and numerical features: Factor Analysis of Mixed Data (FAMD)\n",
    "\n",
    "In Python, these methods can be used in a similar syntax to `sklearn` using the `prince` packages. For more information on the `prince` package, available on `pip` see [the prince documentation on GitHub](https://github.com/MaxHalford/prince) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction for Visualisation\n",
    "\n",
    "PCA is a common and powerful method for dimension reduction, however, there are limitations associated with it. \n",
    "\n",
    "We have seen that reducing the data leads to potential model performance increases, as well and increases in training speed. We can also use dimension reduction as a method for **visualising high dimension data**. \n",
    "\n",
    "Data with large numbers of dimensions (columns) is hard to comprehend, we cannot *see* the data all at once. The natural structure between features will escape us visually if our data has more than three dimensions.\n",
    "\n",
    "By reducing the number of dimensions in our data set we can see some of the relationships between each sample. \n",
    "\n",
    "We can achieve this with PCA, but we will look at another method for doing so. Each method has benefits and challenges associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "One other method to reduce the data dimensionality for visualisation is t-distributed Stochastic Neighbour Embedding (t-SNE).\n",
    "\n",
    "Let's break down the terms in that name:\n",
    "\n",
    "* Stochastic - this method uses random processes\n",
    "* Neighbour - the method uses the affinity between data near each sample\n",
    "* Embedding - the method creates new vector spaces\n",
    "* t-distributed - the method creates joint probabilities of the original dimension affinities\n",
    "\n",
    "In short; the t-SNE method works out the local structures in the data by mapping similar data near each other in the new dimension space. The new dimension space typically has two dimensions.\n",
    "\n",
    "This method uses a *Nearest-Neighbour* search, in a similar way to the KNN algorithm, it is therefore important that we scale the data to be within the same range before we pass it to the t-SNE dimension reducer. \n",
    "\n",
    "(In our case $X$ is already adequately scaled)\n",
    "\n",
    "**Warning: t-SNE can be very slow**\n",
    "\n",
    "Below we are reducing the high dimension data set $X$ used for linear-PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# The random state initialization can significantly impact our output\n",
    "tnse = TSNE(n_components=2, random_state=123)\n",
    "\n",
    "# Fit transform the data into the t-SNE embedding dimensions\n",
    "X_tsne = tnse.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(\"New dimensions using t-SNE\")\n",
    "plt.xlabel(\"$component~1$\")\n",
    "plt.ylabel(\"$component~2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.scatter(x=X_tsne[:,0], y=X_tsne[:,1], c=y, alpha=0.4, cmap=\"coolwarm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at our data which was 100 dimensions and understand *something* about it's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><font size=\"4\">Exercise 7:</font></b> <p> \n",
    "Using \"../../data/bikes.csv\", keep in only the boolean and numeric data, clean the data, visualise the data in 2D using t-SNE and the target \"count\" attribute.\n",
    "</p><p>\n",
    "Compare t-SNE and PCA in 2D for this data. \n",
    "</p><p>\n",
    "What value of $k$ is needed have the total explained variance ratio $> 0.8$ for linear PCA?\n",
    "</p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This chapter has been an introduction into dimension reduction techniques. There are many decisions to make regarding how/whether you reduce the dimensionality of your data. These fall under the general categories of *method selection* and *desired dimension number*.\n",
    "\n",
    "We have covered the theory of lossy compression, this showed how we reduce information using encoding and decoding and the effect this has. We then looked at the linear algebra method of SVD which allowed us to break down our data into informative component matrices. Using this SVD technique we could then analyse the principle components of the data, and subsequently reduce the dimensionality of the data.\n",
    "\n",
    "We then looked at the effects of reducing our data dimensions, which were changes to model prediction performance and training time. \n",
    "\n",
    "Next, there was a discussion of some of the issues with PCA, inlcuding it's linear assumptions and memory requirements, which naturally lead to alternative implementations: \n",
    "\n",
    "* Kernel PCA for non-linear dimension reduction\n",
    "* Incremental PCA for data sets greater than the memory of our machine\n",
    "\n",
    "We also looked at and discussed when to use different Factor Analysis methods.\n",
    "\n",
    "The next section focused on a different method of dimension reduction to achieve useful data visualisation; t-SNE. \n",
    "\n",
    "Dimension reduction can be used in two ways, to gather greater insights into our data with exploratory data analysis, visualisation and for improving our machine learning model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b><font size=\"4\"> Next Chapter: Clustering</font> </b> \n",
    "<p> \n",
    "We have been exploring elements of our training data in this chapter, next we will look into understanding the structure of the data sets in greater depth. We will look at clustering as a way to group our data using a variety of methods that give insight on our data. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
